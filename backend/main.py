from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import numpy as np
from scipy.optimize import curve_fit
import json
import os
import httpx

app = FastAPI(title="Game KPI Projection API", version="2.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Data paths
DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
RAW_DATA_PATH = os.path.join(DATA_DIR, "raw_game_data.json")
CONFIG_PATH = os.path.join(DATA_DIR, "default_config.json")

# Claude API Configuration
# API Keys (í™˜ê²½ë³€ìˆ˜ì—ì„œë§Œ ì½ì–´ì˜´ - ì½”ë“œì— í‚¤ í¬í•¨ ê¸ˆì§€!)
CLAUDE_API_KEY = os.environ.get("CLAUDE_API_KEY", "")
CLAUDE_API_URL = "https://api.anthropic.com/v1/messages"

def load_raw_data():
    with open(RAW_DATA_PATH, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_config():
    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
        return json.load(f)

# Pydantic Models
class RetentionInput(BaseModel):
    selected_games: List[str] = []
    target_d1_retention: Dict[str, float] = {"best": 0.45, "normal": 0.40, "worst": 0.35}

class NRUInput(BaseModel):
    selected_games: List[str] = []
    d1_nru: Dict[str, int] = {"best": 0, "normal": 0, "worst": 0}
    paid_organic_ratio: float = 0.5
    nvr: float = 0.7
    adjustment: Dict[str, float] = {"best_vs_normal": 0.1, "worst_vs_normal": -0.1}
    # V8.5: UA/Brand ì˜ˆì‚° ë¶„ë¦¬
    ua_budget: Optional[int] = 0              # í¼í¬ë¨¼ìŠ¤ ë§ˆì¼€íŒ… ì˜ˆì‚° (ì§ì ‘ ìœ ì…)
    brand_budget: Optional[int] = 0           # ë¸Œëœë”© ì˜ˆì‚° (Organic Boost)
    target_cpa: Optional[int] = 2000          # CPI/CPA (ua_budgetì—ë§Œ ì ìš©)
    base_organic_ratio: Optional[float] = 0.2 # ê¸°ë³¸ ìì—° ìœ ì… ë¹„ìœ¨
    # V8.5+: Pre-Launch & CPA Saturation
    pre_marketing_ratio: Optional[float] = 0.0    # ì‚¬ì „ ë§ˆì¼€íŒ… ë¹„ì¤‘ (0~1, ì˜ˆ: 0.3 = 30%)
    wishlist_conversion_rate: Optional[float] = 0.15  # ìœ„ì‹œë¦¬ìŠ¤íŠ¸/ì‚¬ì „ì˜ˆì•½ â†’ ì‹¤ì œ ìœ ì… ì „í™˜ìœ¨ (PC: 10~20%)
    cpa_saturation_enabled: Optional[bool] = True     # CPA ìƒìŠ¹ ê³„ìˆ˜ í™œì„±í™”
    brand_time_lag_enabled: Optional[bool] = True     # ë¸Œëœë”© ì§€ì—° íš¨ê³¼ í™œì„±í™”

class RevenueInput(BaseModel):
    selected_games_pr: List[str] = []
    selected_games_arppu: List[str] = []
    pr_adjustment: Dict[str, float] = {"best_vs_normal": 0.05, "worst_vs_normal": -0.05}
    arppu_adjustment: Dict[str, float] = {"best_vs_normal": 0.05, "worst_vs_normal": -0.05}

class ProjectionInput(BaseModel):
    launch_date: str
    projection_days: int = 365
    retention: RetentionInput
    nru: NRUInput
    revenue: RevenueInput
    basic_settings: Optional[Dict[str, Any]] = None
    # ë¸”ë Œë”© ì„¤ì •
    blending: Optional[Dict[str, Any]] = None  # { weight: 0.7, genre: "MMORPG", platforms: ["PC"] }
    # V7 ì¶”ê°€: í’ˆì§ˆ ì ìˆ˜, BM íƒ€ì…, ì§€ì—­
    quality_score: Optional[str] = "B"  # S/A/B/C/D
    bm_type: Optional[str] = "Midcore"  # Hardcore/Midcore/Casual/F2P_Cosmetic/Gacha
    regions: Optional[List[str]] = None  # ["korea", "japan", "global", ...]

# ============================================
# ê¸€ë¡œë²Œ ê³„ì ˆì„± íŒ©í„° (ì§€ì—­ë³„ ì›”ê°„ ê°€ì¤‘ì¹˜)
# ============================================
SEASONALITY_BY_REGION = {
    "korea": {1: 1.15, 2: 1.20, 3: 1.00, 4: 0.95, 5: 1.00, 6: 0.95, 7: 1.05, 8: 1.10, 9: 1.00, 10: 1.05, 11: 1.10, 12: 1.15},
    "japan": {1: 1.10, 2: 1.05, 3: 1.05, 4: 1.10, 5: 1.15, 6: 0.95, 7: 1.00, 8: 1.05, 9: 1.00, 10: 1.00, 11: 1.05, 12: 1.20},
    "china": {1: 1.10, 2: 1.25, 3: 1.00, 4: 0.95, 5: 1.05, 6: 1.10, 7: 1.05, 8: 1.00, 9: 1.00, 10: 1.20, 11: 1.15, 12: 1.05},
    "global": {1: 0.95, 2: 0.90, 3: 0.95, 4: 1.00, 5: 1.00, 6: 1.00, 7: 1.05, 8: 1.00, 9: 1.00, 10: 1.05, 11: 1.15, 12: 1.25},
    "sea": {1: 1.05, 2: 1.10, 3: 1.00, 4: 1.00, 5: 1.00, 6: 1.05, 7: 1.05, 8: 1.00, 9: 1.00, 10: 1.00, 11: 1.05, 12: 1.15},
    "na": {1: 0.90, 2: 0.90, 3: 0.95, 4: 1.00, 5: 1.00, 6: 1.05, 7: 1.05, 8: 1.00, 9: 0.95, 10: 1.05, 11: 1.20, 12: 1.25},
    "sa": {1: 1.10, 2: 1.05, 3: 1.00, 4: 0.95, 5: 0.95, 6: 1.00, 7: 1.05, 8: 1.00, 9: 1.00, 10: 1.05, 11: 1.10, 12: 1.15},
    "eu": {1: 0.90, 2: 0.90, 3: 0.95, 4: 1.05, 5: 1.00, 6: 1.00, 7: 1.00, 8: 0.95, 9: 1.00, 10: 1.05, 11: 1.15, 12: 1.25},
}

def calculate_seasonality(regions: List[str], launch_date: str, days: int = 365) -> List[float]:
    """
    ì§€ì—­ë³„ ê³„ì ˆì„± íŒ©í„° ê³„ì‚°
    - ì›”ê°„ ê¸°ë³¸ ê³„ì ˆì„±
    - ì£¼ê°„ ë³€ë™ì„± (ì£¼ë§ +15~20%)
    - íŠ¹ë³„ ì´ë²¤íŠ¸ ìŠ¤íŒŒì´í¬ (ëª…ì ˆ, ëŒ€í˜• ì—…ë°ì´íŠ¸ ë“±)
    """
    from datetime import datetime, timedelta
    import random
    
    try:
        start_date = datetime.strptime(launch_date, "%Y-%m-%d")
    except:
        start_date = datetime(2026, 11, 12)  # ê¸°ë³¸ê°’
    
    # ì‹œë“œ ê³ ì • (ì¬í˜„ì„±)
    random.seed(42)
    
    # íŠ¹ë³„ ì´ë²¤íŠ¸ ë‚ ì§œ (ì›”-ì¼ ê¸°ì¤€)
    SPECIAL_EVENTS = {
        "korea": [(1, 1), (2, 1), (2, 2), (5, 5), (9, 15), (9, 16), (9, 17), (12, 25), (12, 31)],  # ì„¤ë‚ , ì¶”ì„, í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ë“±
        "japan": [(1, 1), (5, 3), (5, 4), (5, 5), (8, 15), (12, 25), (12, 31)],  # ì‹ ì •, ê³¨ë“ ìœ„í¬, ì˜¤ë³¸ ë“±
        "global": [(1, 1), (11, 24), (11, 25), (12, 24), (12, 25), (12, 31)],  # ë¸”ë™í”„ë¼ì´ë°ì´, í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ë“±
        "na": [(1, 1), (7, 4), (11, 24), (11, 25), (12, 24), (12, 25), (12, 31)],
        "eu": [(1, 1), (12, 24), (12, 25), (12, 31)],
        "china": [(1, 1), (2, 1), (2, 2), (10, 1), (10, 2), (10, 3)],  # ì¶˜ì ˆ, êµ­ê²½ì ˆ
        "sea": [(1, 1), (4, 13), (4, 14), (11, 1), (12, 25), (12, 31)],  # ì†¡ë„ë€ ë“±
        "sa": [(1, 1), (2, 13), (2, 14), (12, 25), (12, 31)],  # ì¹´ë‹ˆë°œ ë“±
    }
    
    factors = []
    for day in range(days):
        current_date = start_date + timedelta(days=day)
        month = current_date.month
        weekday = current_date.weekday()  # 0=ì›”, 6=ì¼
        month_day = (current_date.month, current_date.day)
        
        # 1. ì›”ê°„ ê¸°ë³¸ ê³„ì ˆì„±
        region_factors = []
        for region in regions:
            region_key = region.lower()
            if region_key in SEASONALITY_BY_REGION:
                region_factors.append(SEASONALITY_BY_REGION[region_key].get(month, 1.0))
        
        base_factor = np.mean(region_factors) if region_factors else 1.0
        
        # 2. ì£¼ê°„ ë³€ë™ì„± (ê¸ˆ~ì¼ +15~20%, ì›”~í™” -5~10%)
        if weekday == 4:  # ê¸ˆìš”ì¼
            weekly_factor = 1.12 + random.uniform(0, 0.05)
        elif weekday == 5:  # í† ìš”ì¼
            weekly_factor = 1.18 + random.uniform(0, 0.07)
        elif weekday == 6:  # ì¼ìš”ì¼
            weekly_factor = 1.15 + random.uniform(0, 0.05)
        elif weekday in [0, 1]:  # ì›”/í™”
            weekly_factor = 0.92 + random.uniform(0, 0.05)
        else:  # ìˆ˜/ëª©
            weekly_factor = 1.0 + random.uniform(-0.02, 0.02)
        
        # 3. íŠ¹ë³„ ì´ë²¤íŠ¸ ìŠ¤íŒŒì´í¬ (+30~60%)
        event_factor = 1.0
        for region in regions:
            region_key = region.lower()
            if region_key in SPECIAL_EVENTS:
                if month_day in SPECIAL_EVENTS[region_key]:
                    event_factor = max(event_factor, 1.35 + random.uniform(0, 0.25))
        
        # 4. ëŒ€í˜• ì—…ë°ì´íŠ¸ ì‹œë®¬ë ˆì´ì…˜ (30ì¼ë§ˆë‹¤ +20~35%)
        if day > 30 and (day % 30 < 3 or day % 30 > 27):
            event_factor = max(event_factor, 1.20 + random.uniform(0, 0.15))
        
        # 5. ì•½ê°„ì˜ ëœë¤ ë…¸ì´ì¦ˆ (Â±3%)
        noise = 1.0 + random.uniform(-0.03, 0.03)
        
        final_factor = base_factor * weekly_factor * event_factor * noise
        factors.append(final_factor)
    
    return factors

# ============================================
# Time-Decay ë¸”ë Œë”© (ì‹œê°„ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë³€ê²½)
# ============================================
def calculate_time_decay_weight(day: int, days: int = 365) -> float:
    """
    ì‹œê°„ ê°€ì¤‘ì¹˜ ê³„ì‚° (Time-Decay)
    
    D1: ë‚´ë¶€ 90% : ë²¤ì¹˜ë§ˆí¬ 10%
    D180: ë‚´ë¶€ 50% : ë²¤ì¹˜ë§ˆí¬ 50%
    D365: ë‚´ë¶€ 10% : ë²¤ì¹˜ë§ˆí¬ 90%
    
    ì„ í˜• ë³´ê°„ìœ¼ë¡œ ë§¤ì¼ ê°€ì¤‘ì¹˜ ë³€ê²½
    """
    # D1 = 0.9, D365 = 0.1 (ì„ í˜• ê°ì†Œ)
    weight_internal = 0.9 - (0.8 * (day - 1) / (days - 1)) if days > 1 else 0.9
    return max(min(weight_internal, 0.9), 0.1)

def calculate_time_decay_blended_retention(
    internal_curve: List[float],
    benchmark_curve: List[float],
    days: int = 365,
    quality_score: float = 1.0
) -> List[float]:
    """
    Time-Decay ë¸”ë Œë”© ë¦¬í…ì…˜ ì»¤ë¸Œ ìƒì„±
    
    Args:
        internal_curve: ë‚´ë¶€ í‘œë³¸ ë¦¬í…ì…˜ ì»¤ë¸Œ
        benchmark_curve: ë²¤ì¹˜ë§ˆí¬ ë¦¬í…ì…˜ ì»¤ë¸Œ
        days: í”„ë¡œì ì…˜ ê¸°ê°„
        quality_score: í’ˆì§ˆ ì ìˆ˜ (S=1.2, A=1.1, B=1.0, C=0.9, D=0.8)
    """
    blended = []
    for day in range(days):
        weight_internal = calculate_time_decay_weight(day + 1, days)
        weight_benchmark = 1 - weight_internal
        
        internal_val = internal_curve[day] if day < len(internal_curve) else internal_curve[-1]
        benchmark_val = benchmark_curve[day] if day < len(benchmark_curve) else benchmark_curve[-1]
        
        # ë²¤ì¹˜ë§ˆí¬ì— í’ˆì§ˆ ì ìˆ˜ ì ìš©
        adjusted_benchmark = benchmark_val * quality_score
        
        blended_val = (internal_val * weight_internal) + (adjusted_benchmark * weight_benchmark)
        blended.append(max(min(blended_val, 1.0), 0.001))
    
    return blended

# ============================================
# Quality Score ì •ì˜
# ============================================
QUALITY_SCORES = {
    "S": 1.2,   # ìµœìƒê¸‰ (FGT/CBT ê²°ê³¼ ë§¤ìš° ìš°ìˆ˜)
    "A": 1.1,   # ìš°ìˆ˜
    "B": 1.0,   # ë³´í†µ (ê¸°ë³¸ê°’)
    "C": 0.9,   # ë¯¸í¡
    "D": 0.8,   # ë¶€ì§„
}

# ============================================
# BM íƒ€ì…ë³„ ì„¸ë¶„í™” ë²¤ì¹˜ë§ˆí¬
# ============================================
BM_TYPE_MODIFIERS = {
    "Hardcore": {"pr_mod": 0.5, "arppu_mod": 2.0},    # ë‚®ì€ PR, ê³ ì•¡ ARPPU
    "Midcore": {"pr_mod": 1.0, "arppu_mod": 1.0},     # ê¸°ë³¸
    "Casual": {"pr_mod": 2.0, "arppu_mod": 0.4},      # ë†’ì€ PR, ì†Œì•¡ ARPPU
    "F2P_Cosmetic": {"pr_mod": 0.8, "arppu_mod": 0.6}, # ë¬´ë£Œ+ê¾¸ë¯¸ê¸° ì¤‘ì‹¬
    "Gacha": {"pr_mod": 1.5, "arppu_mod": 1.8},       # ê°€ì±  ì¤‘ì‹¬
}
BENCHMARK_DATA = {
    "PC": {
        "MMORPG": {"d1": 0.32, "d7": 0.20, "d30": 0.11, "d90": 0.06, "pr": 0.06, "arppu": 78000},
        "Action RPG": {"d1": 0.30, "d7": 0.18, "d30": 0.09, "d90": 0.04, "pr": 0.05, "arppu": 65000},
        "Battle Royale": {"d1": 0.35, "d7": 0.22, "d30": 0.12, "d90": 0.07, "pr": 0.03, "arppu": 45000},
        "Extraction Shooter": {"d1": 0.28, "d7": 0.16, "d30": 0.08, "d90": 0.04, "pr": 0.04, "arppu": 55000},
        "FPS/TPS": {"d1": 0.33, "d7": 0.20, "d30": 0.10, "d90": 0.05, "pr": 0.04, "arppu": 50000},
        "Strategy": {"d1": 0.25, "d7": 0.15, "d30": 0.08, "d90": 0.04, "pr": 0.07, "arppu": 85000},
        "Casual": {"d1": 0.40, "d7": 0.20, "d30": 0.08, "d90": 0.03, "pr": 0.02, "arppu": 25000},
        "Sports": {"d1": 0.30, "d7": 0.18, "d30": 0.09, "d90": 0.04, "pr": 0.05, "arppu": 60000},
    },
    "Mobile": {
        "MMORPG": {"d1": 0.42, "d7": 0.18, "d30": 0.07, "d90": 0.03, "pr": 0.05, "arppu": 52000},
        "Action RPG": {"d1": 0.38, "d7": 0.15, "d30": 0.06, "d90": 0.02, "pr": 0.04, "arppu": 45000},
        "Battle Royale": {"d1": 0.45, "d7": 0.20, "d30": 0.08, "d90": 0.04, "pr": 0.02, "arppu": 35000},
        "Extraction Shooter": {"d1": 0.35, "d7": 0.14, "d30": 0.05, "d90": 0.02, "pr": 0.03, "arppu": 40000},
        "FPS/TPS": {"d1": 0.40, "d7": 0.17, "d30": 0.07, "d90": 0.03, "pr": 0.03, "arppu": 38000},
        "Strategy": {"d1": 0.35, "d7": 0.16, "d30": 0.07, "d90": 0.03, "pr": 0.06, "arppu": 68000},
        "Casual": {"d1": 0.50, "d7": 0.22, "d30": 0.09, "d90": 0.04, "pr": 0.02, "arppu": 18000},
        "Sports": {"d1": 0.38, "d7": 0.16, "d30": 0.06, "d90": 0.02, "pr": 0.04, "arppu": 42000},
    },
    "Console": {
        "MMORPG": {"d1": 0.35, "d7": 0.22, "d30": 0.12, "d90": 0.06, "pr": 0.05, "arppu": 70000},
        "Action RPG": {"d1": 0.33, "d7": 0.20, "d30": 0.10, "d90": 0.05, "pr": 0.04, "arppu": 60000},
        "Battle Royale": {"d1": 0.38, "d7": 0.24, "d30": 0.13, "d90": 0.07, "pr": 0.02, "arppu": 40000},
        "Extraction Shooter": {"d1": 0.30, "d7": 0.18, "d30": 0.09, "d90": 0.04, "pr": 0.03, "arppu": 50000},
        "FPS/TPS": {"d1": 0.36, "d7": 0.22, "d30": 0.11, "d90": 0.06, "pr": 0.03, "arppu": 48000},
        "Strategy": {"d1": 0.28, "d7": 0.17, "d30": 0.09, "d90": 0.04, "pr": 0.06, "arppu": 75000},
        "Casual": {"d1": 0.42, "d7": 0.20, "d30": 0.08, "d90": 0.03, "pr": 0.02, "arppu": 22000},
        "Sports": {"d1": 0.35, "d7": 0.20, "d30": 0.10, "d90": 0.05, "pr": 0.05, "arppu": 55000},
    }
}

def get_benchmark_data(genre: str, platforms: List[str]) -> Dict[str, float]:
    """ì¥ë¥´/í”Œë«í¼ì— ë§ëŠ” ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë°˜í™˜ (ë‹¤ì¤‘ í”Œë«í¼ì€ í‰ê· )"""
    if not platforms:
        platforms = ["PC"]
    
    values = []
    for platform in platforms:
        if platform in BENCHMARK_DATA and genre in BENCHMARK_DATA[platform]:
            values.append(BENCHMARK_DATA[platform][genre])
    
    if not values:
        # ê¸°ë³¸ê°’ (PC/MMORPG)
        return {"d1": 0.32, "d7": 0.20, "d30": 0.11, "d90": 0.06, "pr": 0.06, "arppu": 78000}
    
    # ë‹¤ì¤‘ í”Œë«í¼ì´ë©´ í‰ê· 
    return {
        "d1": np.mean([v["d1"] for v in values]),
        "d7": np.mean([v["d7"] for v in values]),
        "d30": np.mean([v["d30"] for v in values]),
        "d90": np.mean([v["d90"] for v in values]),
        "pr": np.mean([v["pr"] for v in values]),
        "arppu": np.mean([v["arppu"] for v in values]),
    }

def generate_benchmark_retention_curve(benchmark: Dict[str, float], days: int = 365) -> List[float]:
    """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë¡œ Power Law ë¦¬í…ì…˜ ì»¤ë¸Œ ìƒì„±"""
    # D1, D7, D30, D90 ë°ì´í„°ë¡œ íšŒê·€ë¶„ì„
    x_data = np.array([1, 7, 30, 90])
    y_data = np.array([benchmark["d1"], benchmark["d7"], benchmark["d30"], benchmark["d90"]])
    
    try:
        popt, _ = curve_fit(retention_curve, x_data, y_data, p0=[0.5, -0.3], maxfev=5000)
        a, b = popt
    except:
        a, b = benchmark["d1"], -0.5  # ê¸°ë³¸ê°’
    
    curve = []
    for day in range(1, days + 1):
        ret = a * np.power(day, b)
        ret = max(min(ret, 1.0), 0.001)
        curve.append(ret)
    
    return curve

def calculate_blended_retention(
    internal_curve: List[float],
    benchmark_curve: List[float],
    weight_internal: float
) -> List[float]:
    """ë‚´ë¶€ í‘œë³¸ê³¼ ë²¤ì¹˜ë§ˆí¬ë¥¼ ë¸”ë Œë”©í•œ ë¦¬í…ì…˜ ì»¤ë¸Œ ìƒì„±"""
    weight_benchmark = 1 - weight_internal
    
    blended = []
    for i in range(len(internal_curve)):
        internal_val = internal_curve[i] if i < len(internal_curve) else internal_curve[-1]
        benchmark_val = benchmark_curve[i] if i < len(benchmark_curve) else benchmark_curve[-1]
        blended_val = (internal_val * weight_internal) + (benchmark_val * weight_benchmark)
        blended.append(max(min(blended_val, 1.0), 0.001))
    
    return blended

def calculate_blended_pr(
    internal_pr: List[float],
    benchmark_pr: float,
    weight_internal: float,
    days: int = 365
) -> List[float]:
    """PR ë¸”ë Œë”©"""
    weight_benchmark = 1 - weight_internal
    
    blended = []
    for i in range(days):
        internal_val = internal_pr[i] if i < len(internal_pr) else internal_pr[-1]
        blended_val = (internal_val * weight_internal) + (benchmark_pr * weight_benchmark)
        blended.append(max(min(blended_val, 1.0), 0.001))
    
    return blended

def calculate_blended_arppu(
    internal_arppu: List[float],
    benchmark_arppu: float,
    weight_internal: float,
    days: int = 365
) -> List[float]:
    """ARPPU ë¸”ë Œë”©"""
    weight_benchmark = 1 - weight_internal
    
    blended = []
    for i in range(days):
        internal_val = internal_arppu[i] if i < len(internal_arppu) else internal_arppu[-1]
        blended_val = (internal_val * weight_internal) + (benchmark_arppu * weight_benchmark)
        blended.append(max(blended_val, 1000))
    
    return blended

class AIInsightRequest(BaseModel):
    projection_summary: Dict[str, Any]
    analysis_type: str = "general"  # general, retention, nru, revenue, risk

# Retention Curve: a * (day)^b
def retention_curve(x, a, b):
    return a * np.power(x, b)

def fit_retention_curve(retention_data: List[float]):
    days = np.arange(1, len(retention_data) + 1)
    retention = np.array(retention_data)
    
    valid_mask = (retention > 0) & (retention <= 1)
    if np.sum(valid_mask) < 3:
        return None, None
    
    try:
        popt, _ = curve_fit(
            retention_curve, 
            days[valid_mask], 
            retention[valid_mask],
            p0=[retention_data[0], -0.5],
            bounds=([0, -2], [2, 0]),
            maxfev=5000
        )
        return popt[0], popt[1]
    except:
        return retention_data[0], -0.5

def calculate_retention_coefficients(selected_games: List[str], raw_data: dict):
    retention_games = raw_data['games']['retention']
    
    a_values = []
    b_values = []
    
    for game in selected_games:
        if game in retention_games:
            a, b = fit_retention_curve(retention_games[game])
            if a is not None:
                a_values.append(a)
                b_values.append(b)
    
    if not a_values:
        return 1.0, -0.5
    
    return np.mean(a_values), np.mean(b_values)

def generate_retention_curve(a: float, b: float, target_d1: float, days: int = 365):
    base_d1 = retention_curve(1, a, b)
    if base_d1 > 0:
        scale_factor = target_d1 / base_d1
    else:
        scale_factor = 1.0
    
    curve = []
    for day in range(1, days + 1):
        ret = retention_curve(day, a, b) * scale_factor
        curve.append(min(max(ret, 0.001), 1))
    
    return curve

def calculate_nru_pattern(selected_games: List[str], raw_data: dict):
    nru_games = raw_data['games']['nru']
    
    valid_games = [g for g in selected_games if g in nru_games]
    if not valid_games:
        return [0.98 ** i for i in range(365)]
    
    min_len = min(len(nru_games[g]) for g in valid_games)
    min_len = min(min_len, 365)
    
    daily_ratios = []
    for day in range(1, min_len):
        day_ratios = []
        for game in valid_games:
            data = nru_games[game]
            if day < len(data) and data[day-1] > 0:
                ratio = data[day] / data[day-1]
                if 0 < ratio < 2:
                    day_ratios.append(ratio)
        if day_ratios:
            daily_ratios.append(np.mean(day_ratios))
        else:
            daily_ratios.append(0.98)
    
    while len(daily_ratios) < 364:
        daily_ratios.append(daily_ratios[-1] if daily_ratios else 0.98)
    
    return daily_ratios

def generate_nru_series(total_nru: int, daily_ratios: List[float], days: int = 365, 
                         launch_period: int = 30, sustaining_ratio: float = 0.1):
    """
    NRU ì‹œë¦¬ì¦ˆ ìƒì„± - ëŸ°ì¹­ ë§ˆì¼€íŒ…ì€ D1~D30ì— ì§‘ì¤‘
    
    ğŸ”¥ V8.3 ìˆ˜ì •: Area Normalization ì ìš©
    - total_nru: ëŸ°ì¹­ ê¸°ê°„ ë™ì•ˆì˜ "ì´ ëª¨ê° ìˆ˜" (ì˜ˆì‚°/CPIë¡œ ê³„ì‚°ëœ ê°’)
    - ì´ ì´ëŸ‰ì„ 30ì¼ íŒ¨í„´ì˜ ë©´ì (Area)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ D1 ë†’ì´(Scale)ë¥¼ ì‚°ì¶œ
    - ê²°ê³¼: ì˜ˆì‚° ë²”ìœ„ ë‚´ì—ì„œ ìœ ì €ê°€ ë¶„ì‚° ìœ ì…ë¨
    
    Args:
        total_nru: ëŸ°ì¹­ ê¸°ê°„ ì´ ëª¨ê° ìˆ˜ (ğŸ”¥ ê¸°ì¡´ d1_nru â†’ total_nruë¡œ í•´ì„ ë³€ê²½)
        daily_ratios: ì¼ë³„ ê°ì†Œ ë¹„ìœ¨ (í˜„ì¬ ë¯¸ì‚¬ìš©, í™•ì¥ìš©)
        days: í”„ë¡œì ì…˜ ê¸°ê°„
        launch_period: ëŸ°ì¹­ ë§ˆì¼€íŒ… ì§‘ì¤‘ ê¸°ê°„ (ê¸°ë³¸ 30ì¼)
        sustaining_ratio: ëŸ°ì¹­ í›„ ìœ ì§€ NRU ë¹„ìœ¨ (ê¸°ë³¸ 10%)
    
    Returns:
        ì¼ë³„ NRU ë¦¬ìŠ¤íŠ¸
    """
    nru_series = []
    
    # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: Area Normalization
    # Step 1: ëŸ°ì¹­ ê¸°ê°„ NRU íŒ¨í„´ ìƒì„± (Power Law Decay: 1/t^0.8)
    nru_decay_pattern = []
    for t in range(1, launch_period + 1):
        decay_value = 1.0 / (t ** 0.8)  # D1=1.0, D2=0.57, D3=0.44, ...
        nru_decay_pattern.append(decay_value)
    
    # Step 2: íŒ¨í„´ì˜ ë©´ì (Area) ê³„ì‚° - ì´ëŸ‰ ë³´ì¡´ì˜ ë²•ì¹™!
    pattern_area = sum(nru_decay_pattern)
    
    # Step 3: D1 Scale Factor = ì´ ìœ ì € ìˆ˜ / íŒ¨í„´ ë©´ì 
    # ì´ë ‡ê²Œ í•˜ë©´ ëŸ°ì¹­ ê¸°ê°„ NRUì˜ í•© = total_nruê°€ ë¨
    d1_scale = total_nru / pattern_area if pattern_area > 0 else 0
    
    # Phase 1: ëŸ°ì¹­ ê¸°ê°„ (D1~D30) - ì •ê·œí™”ëœ íŒ¨í„´ ì ìš©
    for day in range(min(launch_period, days)):
        # ì •ê·œí™”ëœ NRU = Scale Ã— íŒ¨í„´ê°’
        daily_nru = int(d1_scale * nru_decay_pattern[day])
        nru_series.append(max(daily_nru, 10))  # ìµœì†Œê°’ 10ìœ¼ë¡œ ì„¤ì •
    
    # Phase 2: ëŸ°ì¹­ í›„ ìœ ì§€ ê¸°ê°„ (D31~D365)
    # D30ì˜ NRUë¥¼ ê¸°ì¤€ìœ¼ë¡œ sustaining_ratioë§Œí¼ ìœ ì§€
    d30_nru = nru_series[-1] if nru_series else 100
    sustaining_nru = int(d30_nru * sustaining_ratio * 10)  # D30ì˜ ~100% ìˆ˜ì¤€ì—ì„œ ì‹œì‘
    
    for day in range(launch_period, days):
        # ìœ ì§€ ê¸°ê°„ì—ë„ ì„œì„œíˆ ê°ì†Œ (ì›” 5% ê°ì†Œ)
        months_after_launch = (day - launch_period) / 30
        decay = np.exp(-0.05 * months_after_launch)
        daily_nru = int(sustaining_nru * decay)
        nru_series.append(max(daily_nru, 10))
    
    return nru_series[:days]


# ============================================
# V8.5: UA/Brand ë¶„ë¦¬ NRU ê³„ì‚° (Organic Boost)
# ============================================
def calculate_organic_boost(brand_budget: int, ua_budget: int) -> float:
    """
    ë¸Œëœë”© ì˜ˆì‚°ì— ë”°ë¥¸ Organic Ratio ì¦í­ ê³„ìˆ˜ ê³„ì‚°
    
    ë¡œì§:
    - brand_budgetì´ ua_budgetì˜ 0%ì¼ ë•Œ: 1.0ë°° (ì¦í­ ì—†ìŒ)
    - brand_budgetì´ ua_budgetì˜ 50%ì¼ ë•Œ: 1.5ë°°
    - brand_budgetì´ ua_budgetì˜ 100%ì¼ ë•Œ: 2.0ë°°
    - brand_budgetì´ ua_budgetì˜ 200%ì¼ ë•Œ: 2.5ë°° (ìˆ˜í™•ì²´ê°)
    
    Logarithmic í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ìˆ˜í™•ì²´ê° íš¨ê³¼ ì ìš©
    """
    if ua_budget <= 0:
        return 1.0
    
    ratio = brand_budget / ua_budget
    # Logarithmic boost: 1 + ln(1 + ratio) * 0.7
    # ratio=0.5 â†’ 1.28ë°°, ratio=1.0 â†’ 1.49ë°°, ratio=2.0 â†’ 1.77ë°°
    boost = 1.0 + np.log(1 + ratio) * 0.7
    return min(boost, 3.0)  # ìµœëŒ€ 3ë°°ë¡œ ìº¡


def generate_nru_series_v85(
    ua_budget: int,
    brand_budget: int, 
    target_cpa: int,
    base_organic_ratio: float,
    days: int = 365,
    launch_period: int = 30,
    sustaining_budget_monthly: int = 0,
    # V8.5+ ì‹ ê·œ íŒŒë¼ë¯¸í„°
    pre_marketing_ratio: float = 0.0,        # ì‚¬ì „ ë§ˆì¼€íŒ… ë¹„ì¤‘
    wishlist_conversion_rate: float = 0.15,  # ìœ„ì‹œë¦¬ìŠ¤íŠ¸ ì „í™˜ìœ¨
    cpa_saturation_enabled: bool = True,     # CPA í¬í™” íš¨ê³¼
    brand_time_lag_enabled: bool = True      # ë¸Œëœë”© ì§€ì—° íš¨ê³¼
) -> tuple:
    """
    V8.5+ NRU ì‹œë¦¬ì¦ˆ ìƒì„± - UA/Brand ë¶„ë¦¬ + Pre-Launch + CPA Saturation
    
    ğŸ”¥ í•µì‹¬ ë¡œì§:
    1. CPA Saturation: ì˜ˆì‚° ê·œëª¨ì— ë”°ë¼ CPA ìƒìŠ¹ (ì‹œì¥ í¬í™” íš¨ê³¼)
    2. Pre-Launch Reservoir: ì‚¬ì „ì˜ˆì•½/ìœ„ì‹œë¦¬ìŠ¤íŠ¸ ìœ ì €ë¥¼ D1ì— í­ë°œì  ìœ ì…
    3. Brand Time-Lag: ë¸Œëœë”© íš¨ê³¼ê°€ ì„œì„œíˆ ë‚˜íƒ€ë‚˜ê³  ì”ì¡´
    
    Args:
        ua_budget: í¼í¬ë¨¼ìŠ¤ ë§ˆì¼€íŒ… ì˜ˆì‚° (ì§ì ‘ ìœ ì…)
        brand_budget: ë¸Œëœë”© ì˜ˆì‚° (Organic Boost)
        target_cpa: CPI/CPA ë‹¨ê°€
        base_organic_ratio: ê¸°ë³¸ ìì—° ìœ ì… ë¹„ìœ¨
        days: í”„ë¡œì ì…˜ ê¸°ê°„
        launch_period: ëŸ°ì¹­ ë§ˆì¼€íŒ… ì§‘ì¤‘ ê¸°ê°„
        sustaining_budget_monthly: ì›”ê°„ ìœ ì§€ ë§ˆì¼€íŒ… ì˜ˆì‚°
        pre_marketing_ratio: ì‚¬ì „ ë§ˆì¼€íŒ… ë¹„ì¤‘ (0~1)
        wishlist_conversion_rate: ìœ„ì‹œë¦¬ìŠ¤íŠ¸/ì‚¬ì „ì˜ˆì•½ ì „í™˜ìœ¨
        cpa_saturation_enabled: CPA ìƒìŠ¹ ê³„ìˆ˜ í™œì„±í™”
        brand_time_lag_enabled: ë¸Œëœë”© ì§€ì—° íš¨ê³¼ í™œì„±í™”
    
    Returns:
        (nru_series, paid_nru_total, organic_nru_total, organic_boost, meta_info)
    """
    import math
    
    # ============================================
    # 1. CPA Saturation Effect (ì‹œì¥ í¬í™”)
    # ============================================
    # ì˜ˆì‚°ì´ í´ìˆ˜ë¡ íš¨ìœ¨ ì¢‹ì€ ìœ ì €ê°€ ê³ ê°ˆë˜ì–´ CPA ìƒìŠ¹
    # ê³µì‹: Effective CPA = Target CPA Ã— (1 + (Budget / 5ì–µ) Ã— 0.05)
    if cpa_saturation_enabled and ua_budget > 0:
        saturation_factor = 1 + (ua_budget / 500_000_000) * 0.05
        effective_cpa = int(target_cpa * saturation_factor)
    else:
        saturation_factor = 1.0
        effective_cpa = target_cpa
    
    # ============================================
    # 2. UA/Brand ì˜ˆì‚° ë¶„ë¦¬ ë° NRU ê³„ì‚°
    # ============================================
    # 2-1. Pre-Launch ì˜ˆì‚°ê³¼ Post-Launch ì˜ˆì‚° ë¶„ë¦¬
    pre_launch_ua = int(ua_budget * pre_marketing_ratio)
    post_launch_ua = ua_budget - pre_launch_ua
    
    # 2-2. Paid NRU ê³„ì‚° (Effective CPA ì ìš©)
    pre_launch_paid_nru = pre_launch_ua // effective_cpa if effective_cpa > 0 else 0
    post_launch_paid_nru = post_launch_ua // effective_cpa if effective_cpa > 0 else 0
    
    # 2-3. Organic Boost Factor ê³„ì‚° (Brand Budget ê¸°ë°˜)
    organic_boost = calculate_organic_boost(brand_budget, ua_budget)
    
    # 2-4. Organic NRU ê³„ì‚°
    total_paid_nru = pre_launch_paid_nru + post_launch_paid_nru
    effective_organic_ratio = base_organic_ratio * organic_boost
    organic_nru_total = int(total_paid_nru * effective_organic_ratio)
    
    # ============================================
    # 3. Pre-Launch Reservoir (ì‚¬ì „ì˜ˆì•½/ìœ„ì‹œë¦¬ìŠ¤íŠ¸)
    # ============================================
    # ì‚¬ì „ ë§ˆì¼€íŒ…ìœ¼ë¡œ ëª¨ì€ ìœ ì € = "ì €ìˆ˜ì§€"ì— ë‹´ì•„ë’€ë‹¤ê°€ D1ì— í„°ëœ¨ë¦¼
    # ìœ„ì‹œë¦¬ìŠ¤íŠ¸ ì „í™˜ìœ¨ ì ìš© (PC: 10~20%, Mobile: 15~25%)
    wishlist_users = int(pre_launch_paid_nru / wishlist_conversion_rate) if wishlist_conversion_rate > 0 else 0
    d1_burst_users = int(wishlist_users * wishlist_conversion_rate)  # ì‹¤ì œ D1 ìœ ì…
    
    # D1~D3 ë²„ìŠ¤íŠ¸ ë¶„ë°°: D1=80%, D2=10%, D3=10%
    burst_distribution = [0.80, 0.10, 0.10]
    
    # ============================================
    # 4. Brand Time-Lag Effect (ë¸Œëœë”© ì§€ì—° íš¨ê³¼)
    # ============================================
    # ë¸Œëœë”© íš¨ê³¼ëŠ” Bell Curveë¡œ ì„œì„œíˆ ë‚˜íƒ€ë‚˜ê³  ì”ì¡´
    # D-30 ~ D+60 êµ¬ê°„ì— ì •ê·œë¶„í¬ë¡œ ë¶„ì‚°
    brand_effect_curve = []
    if brand_time_lag_enabled and brand_budget > 0:
        # ì •ê·œë¶„í¬ (í‰ê· =15, í‘œì¤€í¸ì°¨=20) â†’ D1~D60 êµ¬ê°„ì— íš¨ê³¼ ë¶„í¬
        for day in range(days):
            # Bell curve centered at D15 with spread of 20 days
            effect = math.exp(-0.5 * ((day - 15) / 20) ** 2)
            brand_effect_curve.append(effect)
        # ì •ê·œí™”
        total_effect = sum(brand_effect_curve)
        brand_effect_curve = [e / total_effect for e in brand_effect_curve] if total_effect > 0 else [0] * days
    else:
        # Time-Lag ë¹„í™œì„±í™” ì‹œ ì¦‰ì‹œ íš¨ê³¼
        brand_effect_curve = [1.0 / 30 if i < 30 else 0 for i in range(days)]
    
    # ============================================
    # 5. NRU ì‹œë¦¬ì¦ˆ ìƒì„± (í†µí•©)
    # ============================================
    nru_series = [0] * days
    
    # 5-1. Pre-Launch Burst (D1~D3 í­ë°œ)
    for i, ratio in enumerate(burst_distribution):
        if i < days:
            nru_series[i] += int(d1_burst_users * ratio)
    
    # 5-2. Post-Launch UA (ëŸ°ì¹­ í›„ í¼í¬ë¨¼ìŠ¤ ë§ˆì¼€íŒ…)
    # Area Normalizationìœ¼ë¡œ 30ì¼ê°„ ë¶„ë°°
    nru_decay_pattern = [1.0 / (t ** 0.8) for t in range(1, launch_period + 1)]
    pattern_area = sum(nru_decay_pattern)
    d1_scale = post_launch_paid_nru / pattern_area if pattern_area > 0 else 0
    
    for day in range(min(launch_period, days)):
        daily_nru = int(d1_scale * nru_decay_pattern[day])
        nru_series[day] += max(daily_nru, 0)
    
    # 5-3. Organic NRU (Brand Time-Lag ì ìš©)
    for day in range(days):
        organic_daily = int(organic_nru_total * brand_effect_curve[day])
        nru_series[day] += organic_daily
    
    # 5-4. Sustaining ê¸°ê°„ (D31~D365)
    # [FIX] Sustainingì€ ë¹„ìš©ìœ¼ë¡œë§Œ ì²˜ë¦¬, NRUëŠ” ìµœì†Œí•œìœ¼ë¡œ ìœ ì§€
    # ì›” ë§¤ì¶œì˜ 7%ë¥¼ Sustainingì— ì“°ì§€ë§Œ, ì´ëŠ” ROAS ê³„ì‚°ì—ë§Œ ë°˜ì˜
    # ì‹¤ì œ NRUëŠ” ìì—° ê°ì‡  (D30 ëŒ€ë¹„ ê¸‰ê²©íˆ ê°ì†Œ)
    d30_nru = nru_series[29] if len(nru_series) > 29 else 100
    
    # Sustaining NRUëŠ” D30ì˜ 5% ìˆ˜ì¤€ì—ì„œ ì‹œì‘, ë¹ ë¥´ê²Œ ê°ì‡ 
    base_sustaining_nru = int(d30_nru * 0.05)  # D30ì˜ 5% (ê¸°ì¡´ 20%ì—ì„œ í¬ê²Œ ì¶•ì†Œ)
    
    for day in range(launch_period, days):
        months_after_launch = (day - launch_period) / 30
        # [FIX] ë” ê°€íŒŒë¥¸ ê°ì‡ ìœ¨ ì ìš© (ì›” 10% ê°ì†Œ â†’ 6ê°œì›” í›„ ~53%, 12ê°œì›” í›„ ~28%)
        decay = np.exp(-0.1 * months_after_launch)
        daily_nru = int(base_sustaining_nru * decay)
        nru_series[day] += max(daily_nru, 5)  # ìµœì†Œê°’ 10 â†’ 5ë¡œ ì¶•ì†Œ
    
    # ìµœì†Œê°’ ë³´ì¥ (5ëª… ì´í•˜ë¡œ ë–¨ì–´ì§€ì§€ ì•ŠìŒ)
    nru_series = [max(nru, 5) for nru in nru_series]
    
    # ============================================
    # 6. ë©”íƒ€ ì •ë³´ ë°˜í™˜
    # ============================================
    meta_info = {
        "effective_cpa": effective_cpa,
        "cpa_saturation_factor": round(saturation_factor, 3),
        "pre_launch_users": pre_launch_paid_nru,
        "wishlist_users": wishlist_users,
        "d1_burst_users": d1_burst_users,
        "post_launch_paid_nru": post_launch_paid_nru,
        "organic_boost_factor": round(organic_boost, 2),
        "brand_time_lag_peak_day": 15 if brand_time_lag_enabled else 1
    }
    
    return nru_series[:days], total_paid_nru, organic_nru_total, organic_boost, meta_info

def calculate_pr_pattern(selected_games: List[str], raw_data: dict):
    pr_games = raw_data['games']['payment_rate']
    
    valid_games = [g for g in selected_games if g in pr_games]
    if not valid_games:
        return [0.02] * 365
    
    min_len = min(len(pr_games[g]) for g in valid_games)
    min_len = min(min_len, 365)
    
    pattern = []
    for day in range(min_len):
        day_values = [pr_games[g][day] for g in valid_games if day < len(pr_games[g]) and pr_games[g][day] > 0]
        avg_pr = np.mean(day_values) if day_values else 0.02
        pattern.append(max(avg_pr, 0.001))
    
    while len(pattern) < 365:
        pattern.append(pattern[-1] if pattern else 0.02)
    
    return pattern[:365]

def calculate_arppu_pattern(selected_games: List[str], raw_data: dict):
    arppu_games = raw_data['games']['arppu']
    
    valid_games = [g for g in selected_games if g in arppu_games]
    if not valid_games:
        return [50000] * 365
    
    min_len = min(len(arppu_games[g]) for g in valid_games)
    min_len = min(min_len, 365)
    
    pattern = []
    for day in range(min_len):
        day_values = [arppu_games[g][day] for g in valid_games if day < len(arppu_games[g]) and arppu_games[g][day] > 0]
        avg_arppu = np.mean(day_values) if day_values else 50000
        pattern.append(max(avg_arppu, 1000))
    
    while len(pattern) < 365:
        pattern.append(pattern[-1] if pattern else 50000)
    
    return pattern[:365]

def calculate_dau_matrix(nru_series: List[int], retention_curve: List[float], days: int = 365):
    """
    [R1 Fix] DAU ì½”í˜¸íŠ¸ ê³„ì‚°
    - D0 (ì„¤ì¹˜ ë‹¹ì¼): ë¦¬í…ì…˜ = 1.0 (100%)
    - D1 ì´í›„: retention_curve[days_since_install - 1]
    """
    daily_dau = []
    
    for active_day in range(days):
        total_dau = 0
        for cohort_day in range(active_day + 1):
            days_since_install = active_day - cohort_day
            
            if cohort_day < len(nru_series):
                nru = nru_series[cohort_day]
                
                if days_since_install == 0:
                    retention = 1.0
                else:
                    idx = days_since_install - 1
                    retention = retention_curve[idx] if idx < len(retention_curve) else 0
                
                total_dau += nru * retention
        daily_dau.append(int(total_dau))
    
    return daily_dau

def calculate_revenue(dau: List[float], pr: List[float], arppu: List[float]):
    """
    ì¼ë³„ ë§¤ì¶œ ê³„ì‚°
    
    Revenue = DAU Ã— PR Ã— (ARPPU / 30)
    
    ì£¼ì˜: ARPPUëŠ” 'ì›”ê°„' ê²°ì œìë‹¹ í‰ê·  ê²°ì œì•¡ì´ë¯€ë¡œ,
          ì¼ë³„ ê³„ì‚° ì‹œ 30ìœ¼ë¡œ ë‚˜ëˆ ì•¼ í•¨
    """
    revenue = []
    for i in range(len(dau)):
        pr_val = pr[i] if i < len(pr) else pr[-1]
        arppu_val = arppu[i] if i < len(arppu) else arppu[-1]
        
        # ARPPUë¥¼ ì¼ë³„ë¡œ í™˜ì‚° (ì›”ê°„ ARPPU / 30)
        daily_arppu = arppu_val / 30
        
        # ì¼ë³„ ë§¤ì¶œ = DAU Ã— PR Ã— ì¼ë³„ ARPPU
        daily_revenue = dau[i] * pr_val * daily_arppu
        revenue.append(daily_revenue)
    
    return revenue

# Claude AI Integration
# V9.8: ì•ˆì „í•œ ëª¨ë¸ëª… ì„¤ì • (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë¸)
CURRENT_MODEL = "claude-3-5-sonnet-20240620"  # âœ… ì‹¤ì œ ì‘ë™í•˜ëŠ” ìµœì‹  ëª¨ë¸

async def get_claude_insight(prompt: str) -> str:
    """Call Claude API for AI insights with Mock Fallback"""
    if not CLAUDE_API_KEY:
        print("ğŸ’¡ API Keyê°€ ì—†ìŠµë‹ˆë‹¤. Mock ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return None  # Mockìœ¼ë¡œ í´ë°±
    
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                CLAUDE_API_URL,
                headers={
                    "x-api-key": CLAUDE_API_KEY,
                    "anthropic-version": "2023-06-01",
                    "content-type": "application/json"
                },
                json={
                    "model": CURRENT_MODEL,  # âœ… ì˜¬ë°”ë¥¸ ëª¨ë¸ëª… ì‚¬ìš©
                    "max_tokens": 2000,
                    "messages": [
                        {"role": "user", "content": prompt}
                    ]
                }
            )
            
            # API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
            response.raise_for_status()
            
            data = response.json()
            return data["content"][0]["text"]
                
    except httpx.HTTPStatusError as e:
        print(f"âŒ API HTTP ì—ëŸ¬: {e.response.status_code}")
        return None
    except Exception as e:
        print(f"âŒ AI í˜¸ì¶œ ì—ëŸ¬: {str(e)}")
        print("ğŸ”„ ì•ˆì „í•˜ê²Œ Mock ë°ì´í„°ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        return None

def create_insight_prompt(summary: Dict[str, Any], analysis_type: str) -> str:
    """Create prompt for Claude based on analysis type with Multi-Persona approach"""
    
    # V7 ì„¤ì • ì •ë³´ ì¶”ì¶œ
    v7_settings = summary.get('v7_settings', {})
    blending = summary.get('blending', {})
    
    # V9.2: í”Œë«í¼ë³„ ìš©ì–´ ë™ì  ì„¤ì •
    platforms = blending.get('platforms', ['PC'])
    cost_metric = "CPI" if "Mobile" in platforms else "CPA"
    is_pc_console = any(p in ['PC', 'Console'] for p in platforms)
    
    # V9.2: BEP ìƒíƒœ ê³„ì‚°
    bep_day = summary.get('bep_day', -1)
    bep_status = ""
    if bep_day <= 0:
        bep_status = f"""
[âš ï¸ Critical Issue: BEP ë¯¸ë‹¬ì„±]
í˜„ì¬ êµ¬ì¡°ë¡œëŠ” 1ë…„ ë‚´ íˆ¬ì íšŒìˆ˜ê°€ ì–´ë µìŠµë‹ˆë‹¤. ë¶„ì„ ì‹œ ë‹¤ìŒ ì „ëµì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”:
1. {cost_metric} ì ˆê° ë°©ì•ˆ: íƒ€ê²ŸíŒ… ìµœì í™” ë˜ëŠ” ì˜¤ê°€ë‹‰ ë¹„ì¤‘ í™•ëŒ€
2. LTV ê°œì„ : ë¦¬í…ì…˜ D30ì„ 5%p ì˜¬ë¦¬ê±°ë‚˜ ARPPUë¥¼ 15% ìƒí–¥í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜ ì œì•ˆ
3. BM ì¬ê²€í† : íŒ¨í‚¤ì§€ ê°€ê²© ë˜ëŠ” ì¸ê²Œì„ ê²°ì œ ëª¨ë¸ ì¡°ì •"""
    else:
        bep_status = f"BEPëŠ” D+{bep_day}ì— ë‹¬ì„±ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. ì•ˆì •ì ì¸ í˜„ê¸ˆ íë¦„ì´ ê¸°ëŒ€ë©ë‹ˆë‹¤."
    
    base_context = f"""ë‹¹ì‹ ì€ ê²Œì„ KPI í”„ë¡œì ì…˜ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” 4ëª…ì˜ ì „ë¬¸ê°€ íŒ¨ë„ì…ë‹ˆë‹¤.

[ì „ë¬¸ê°€ íŒ¨ë„ êµ¬ì„±]
1. UA ë° ë¸Œëœë”© ë§ˆì¼€í„° ì „ë¬¸ê°€: {cost_metric} ì ì •ì„±, ëª¨ê° íš¨ìœ¨, UA ì „ëµ, CAC/LTV ë¶„ì„, Organic Boost í‰ê°€
2. ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì „ë¬¸ê°€: ì§€í‘œ ê±´ì „ì„±, ë¦¬í…ì…˜ íŒ¨í„´, í†µê³„ì  ì‹ ë¢°ë„, ì˜ˆì¸¡ ì •í™•ë„
3. í¼ë¸”ë¦¬ì‹± ì „ë¬¸ê°€: BM êµ¬ì¡°, ì‹œì¥ ê²½ìŸë ¥, ì¥ë¥´ íŠ¹ì„±, ê¸€ë¡œë²Œ íŠ¸ë Œë“œ, ëŸ°ì¹­ íƒ€ì´ë°
4. ë¼ì´ë¸Œ ì„œë¹„ìŠ¤ ì „ë¬¸ê°€: BEP, ROAS, íˆ¬ì íšŒìˆ˜, Sustaining ì „ëµ, ì½˜í…ì¸  ìš´ì˜

[í”Œë«í¼ ì»¨í…ìŠ¤íŠ¸]
- í”Œë«í¼: {', '.join(platforms)}
- ë¹„ìš© ì§€í‘œ: {cost_metric} ({'PC/Consoleì€ ì„¤ì¹˜ë‹¹ ë¹„ìš©ì´ ì•„ë‹Œ ì „í™˜ë‹¹ ë¹„ìš© ê¸°ì¤€' if is_pc_console else 'ëª¨ë°”ì¼ ì„¤ì¹˜ë‹¹ ë¹„ìš© ê¸°ì¤€'})
{'- ì°¸ê³ : PC/Console í”Œë«í¼ì€ CPI ê¸°ë°˜ UAê°€ ì œí•œì ì´ë¯€ë¡œ Steam ë…¸ì¶œ, ë¯¸ë””ì–´ ë¦¬ë·°, ì»¤ë®¤ë‹ˆí‹° ë°”ì´ëŸ´ ë“± Organic ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.' if is_pc_console else ''}

[BEP ìƒíƒœ]
{bep_status}

[í”„ë¡œì ì…˜ ê²°ê³¼ ìš”ì•½]
í”„ë¡œì ì…˜ ê¸°ê°„: {summary.get('projection_days', 365)}ì¼
ëŸ°ì¹­ì¼: {summary.get('launch_date', 'N/A')}

Best ì‹œë‚˜ë¦¬ì˜¤:
- ì´ Gross Revenue: {summary.get('best', {}).get('gross_revenue', 0):,.0f}ì›
- ì´ NRU: {summary.get('best', {}).get('total_nru', 0):,}ëª…
- Peak DAU: {summary.get('best', {}).get('peak_dau', 0):,}ëª…
- í‰ê·  DAU: {summary.get('best', {}).get('average_dau', 0):,}ëª…

Normal ì‹œë‚˜ë¦¬ì˜¤:
- ì´ Gross Revenue: {summary.get('normal', {}).get('gross_revenue', 0):,.0f}ì›
- ì´ NRU: {summary.get('normal', {}).get('total_nru', 0):,}ëª…
- Peak DAU: {summary.get('normal', {}).get('peak_dau', 0):,}ëª…
- í‰ê·  DAU: {summary.get('normal', {}).get('average_dau', 0):,}ëª…

Worst ì‹œë‚˜ë¦¬ì˜¤:
- ì´ Gross Revenue: {summary.get('worst', {}).get('gross_revenue', 0):,.0f}ì›
- ì´ NRU: {summary.get('worst', {}).get('total_nru', 0):,}ëª…
- Peak DAU: {summary.get('worst', {}).get('peak_dau', 0):,}ëª…
- í‰ê·  DAU: {summary.get('worst', {}).get('average_dau', 0):,}ëª…

[V7 ì‚°ìˆ  ê·¼ê±° - ì´ ê²°ê³¼ê°€ ì–´ë–»ê²Œ ë„ì¶œë˜ì—ˆëŠ”ì§€]
- ë¸”ë Œë”© ë¹„ìœ¨: ë‚´ë¶€ í‘œë³¸ {blending.get('weight_internal', 0.7)*100:.0f}% + ë²¤ì¹˜ë§ˆí¬ {blending.get('weight_benchmark', 0.3)*100:.0f}%
- Time-Decay: {blending.get('time_decay', True)} (D1:ë‚´ë¶€90% â†’ D365:ë²¤ì¹˜ë§ˆí¬90%)
- í’ˆì§ˆ ë“±ê¸‰: {v7_settings.get('quality_score', 'B')}ê¸‰ (ìŠ¹ìˆ˜ Ã—{v7_settings.get('quality_multiplier', 1.0)})
- BM íƒ€ì…: {v7_settings.get('bm_type', 'Midcore')}
- ì ìš© ì§€ì—­: {', '.join(v7_settings.get('regions', ['global']))}
- ê³„ì ˆì„± ì ìš©: {v7_settings.get('seasonality_applied', True)}
- ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€: {blending.get('genre', 'N/A')} / {', '.join(platforms)}

[ì¤‘ìš” ì§€ì‹œì‚¬í•­]
- ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•(###, **, -, * ë“±)ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”
- ë²ˆí˜¸ëŠ” 1. 2. 3. í˜•ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”
- ê°•ì¡°ëŠ” ë”°ì˜´í‘œë‚˜ ê´„í˜¸ë¡œ í‘œí˜„í•˜ì„¸ìš”
- ì˜ì‚¬ê²°ì • ì§€ì›ìš©ìœ¼ë¡œ ì „ë¬¸ì ì´ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
- ì¥ë¥´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ë°˜ì˜í•˜ì„¸ìš” (ì…ë ¥ëœ ì¥ë¥´: {blending.get('genre', 'N/A')})
- BEP ë¯¸ë‹¬ì„± ì‹œ ë°˜ë“œì‹œ ê°œì„  ì „ëµì„ í¬í•¨í•˜ì„¸ìš”
"""
    
    type_prompts = {
        "executive_report": f"""
[ë¶„ì„ ìš”ì²­: ì¢…í•©ë¶„ì„ ë³´ê³ ì„œ]
4ëª…ì˜ ì „ë¬¸ê°€ê°€ ê°ìì˜ ê´€ì ì—ì„œ ë¶„ì„í•˜ê³ , ìµœì¢… ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ ì¢…í•© ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

í”Œë«í¼: {', '.join(blending.get('platforms', ['PC']))}
{'- PC/Console í”Œë«í¼: CPI/CPA ê¸°ë°˜ UAê°€ ì œí•œì ì´ë¯€ë¡œ Steam ë…¸ì¶œ, ë¯¸ë””ì–´ ë¦¬ë·°, ì»¤ë®¤ë‹ˆí‹° ë°”ì´ëŸ´ ë“± Organic ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.' if any(p in ['PC', 'Console'] for p in blending.get('platforms', ['PC'])) else ''}

ì‘ë‹µ í˜•ì‹:

[1. Executive Summary - í•µì‹¬ ìš”ì•½]
Normal ì‹œë‚˜ë¦¬ì˜¤ ê¸°ì¤€ 1ë…„ ì˜ˆìƒ ë§¤ì¶œê³¼ í•µì‹¬ ì§€í‘œë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½

[2. ì‚°ìˆ  ê·¼ê±° ë° ê°€ì •]
- ì´ í”„ë¡œì ì…˜ì´ ì–´ë–¤ ê°€ì •ê³¼ ë¡œì§ìœ¼ë¡œ ë„ì¶œë˜ì—ˆëŠ”ì§€ ì„¤ëª…
- ë¸”ë Œë”© ë¹„ìœ¨, í’ˆì§ˆ ë“±ê¸‰, BM íƒ€ì…ì´ ê²°ê³¼ì— ë¯¸ì¹œ ì˜í–¥

[3. ì „ë¬¸ê°€ í†µí•© ë¶„ì„]
UA&ë¸Œëœë”© ë§ˆì¼€í„°, í¼ë¸”ë¦¬ì‹±, ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤, ë¼ì´ë¸Œ ì„œë¹„ìŠ¤ 4ëª…ì˜ ì „ë¬¸ê°€ ê´€ì ì„ ì¢…í•©í•˜ì—¬ ë‹¤ìŒ ì‚¬í•­ì„ í•˜ë‚˜ì˜ í†µí•©ëœ ë¶„ì„ìœ¼ë¡œ ì‘ì„±:
- ëª¨ê° íš¨ìœ¨ ë° {'Organic ì¤‘ì‹¬ ë§ˆì¼€íŒ… ì „ëµ' if any(p in ['PC', 'Console'] for p in blending.get('platforms', ['PC'])) else 'UA ì „ëµ'}
- {blending.get('genre', 'N/A')} ì¥ë¥´ ì‹œì¥ ê²½ìŸë ¥ ë° BM êµ¬ì¡° ì í•©ì„±
- ë¦¬í…ì…˜ ì»¤ë¸Œ ê±´ì „ì„± ë° Best-Worst í¸ì°¨ ({((summary.get('best', {{}}).get('gross_revenue', 1) / max(summary.get('worst', {{}}).get('gross_revenue', 1), 1) - 1) * 100):.0f}%)
- ROAS, ì†ìµë¶„ê¸°ì , Sustaining ì „ëµ

ê° ì „ë¬¸ê°€ì˜ ì˜ê²¬ì„ ë‚˜ì—´í•˜ì§€ ë§ê³ , í•˜ë‚˜ì˜ í†µí•©ëœ ë¬¸ë‹¨ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.

[4. í”„ë¡œì ì…˜ ì‹ ë¢°ë„ í‰ê°€]
- ë°ì´í„° ì‹ ë¢°ë„: (í‘œë³¸ ìˆ˜, ë²¤ì¹˜ë§ˆí¬ ì •í•©ì„±)
- ê°€ì •ì˜ í˜„ì‹¤ì„±: ({'Organic ë¹„ìœ¨' if any(p in ['PC', 'Console'] for p in blending.get('platforms', ['PC'])) else 'CPI'}, ë¦¬í…ì…˜, ARPU ê°€ì • ì ì •ì„±)
- í¸ì°¨ ë¶„ì„: Best-Worst ì‹œë‚˜ë¦¬ì˜¤ ê°„ í¸ì°¨ ë¶„ì„

[5. ë¦¬ìŠ¤í¬ ë¶„ì„ ë° BEP ë‹¬ì„± ì „ëµ]
- í•µì‹¬ ë¦¬ìŠ¤í¬ 3ê°€ì§€ì™€ ì™„í™” ì „ëµ
- BEP(ì†ìµë¶„ê¸°ì ) ë‹¬ì„±ì´ ì–´ë ¤ìš´ ê²½ìš°: ë‹¬ì„±ì„ ìœ„í•œ êµ¬ì²´ì  ê°œì„  ë°©ì•ˆ ì œì‹œ (ë§ˆì¼€íŒ… íš¨ìœ¨í™”, ë¦¬í…ì…˜ ê°œì„ , ARPU í–¥ìƒ ë“±)

[6. ê²½ìŸë ¥ ë¶„ì„]
- {blending.get('genre', 'N/A')} ì¥ë¥´ ì‹œì¥ ë‚´ ì˜ˆìƒ í¬ì§€ì…”ë‹

[7. Go/No-Go ê¶Œê³ ]
- ìµœì¢… ê¶Œê³ : (Go / Conditional Go / No-Go ì¤‘ í•˜ë‚˜)
- ê¶Œê³  ì´ìœ : í•œ ë¬¸ì¥
- ê¶Œì¥ ì•¡ì…˜ 3ê°€ì§€

ì´ 1200ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
        "general": """
[ë¶„ì„ ìš”ì²­: ì¢…í•© ë¶„ì„]
4ëª…ì˜ ì „ë¬¸ê°€(UA&ë¸Œëœë”© ë§ˆì¼€í„°, í¼ë¸”ë¦¬ì‹±, ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤, ë¼ì´ë¸Œ ì„œë¹„ìŠ¤)ì˜ ê´€ì ì„ ì¢…í•©í•œ í†µí•© ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
1. í†µí•© ë¶„ì„: ëª¨ê° íš¨ìœ¨, ì‹œì¥ ê²½ìŸë ¥, ì§€í‘œ ê±´ì „ì„±, ìš´ì˜ ë° íˆ¬ì íšŒìˆ˜ ê´€ì ì„ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í†µí•©í•˜ì—¬ ì‘ì„± (ê° ì „ë¬¸ê°€ ì˜ê²¬ì„ ë‚˜ì—´í•˜ì§€ ë§ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°)
2. í•µì‹¬ ê°•ì  2ê°€ì§€
3. í•µì‹¬ ë¦¬ìŠ¤í¬ 2ê°€ì§€
4. ê¶Œì¥ ì•¡ì…˜ 3ê°€ì§€

ì´ 400ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
        "reliability": f"""
[ë¶„ì„ ìš”ì²­: ì‹ ë¢°ë„ í‰ê°€]
4ëª…ì˜ ì „ë¬¸ê°€(UA&ë¸Œëœë”© ë§ˆì¼€í„°, í¼ë¸”ë¦¬ì‹±, ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤, ë¼ì´ë¸Œ ì„œë¹„ìŠ¤)ê°€ ì´ í”„ë¡œì ì…˜ì˜ ì‹ ë¢°ë„ë¥¼ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

í”Œë«í¼: {', '.join(blending.get('platforms', ['PC']))}
{'- PC/Console í”Œë«í¼ì€ ëª¨ë°”ì¼ê³¼ ë‹¬ë¦¬ CPI/CPA ê¸°ë°˜ UAê°€ ì œí•œì ì´ë¯€ë¡œ, Steam/ìŠ¤í† ì–´ ë…¸ì¶œ, ë¯¸ë””ì–´ ë¦¬ë·°, ì»¤ë®¤ë‹ˆí‹° ë°”ì´ëŸ´ ë“± Organic ì¤‘ì‹¬ ëª¨ê°ì„ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.' if any(p in ['PC', 'Console'] for p in blending.get('platforms', ['PC'])) else '- ëª¨ë°”ì¼ í”Œë«í¼ì€ CPI/CPA ê¸°ë°˜ UA íš¨ìœ¨ì„ ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.'}

ì‘ë‹µ í˜•ì‹:
1. ì‹ ë¢°ë„ ì ìˆ˜: (100ì  ë§Œì , ìˆ«ìë§Œ)
2. ì‹ ë¢°ë„ ë“±ê¸‰: (A/B/C/D/F ì¤‘ í•˜ë‚˜)
3. í†µí•© ì‹ ë¢°ë„ í‰ê°€: {'ëª¨ê° ëª©í‘œ í˜„ì‹¤ì„± (Organic ì¤‘ì‹¬), ' if any(p in ['PC', 'Console'] for p in blending.get('platforms', ['PC'])) else 'NRU/CPI ëª©í‘œ í˜„ì‹¤ì„±, '}í‘œë³¸ ë°ì´í„° í’ˆì§ˆ, ì‹œì¥ ë²¤ì¹˜ë§ˆí¬ ì ì •ì„±, ìˆ˜ìµ ì˜ˆì¸¡ í˜„ì‹¤ì„±ì„ í•˜ë‚˜ì˜ í†µí•©ëœ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„ì„
4. ì‹ ë¢°ë„ í–¥ìƒ ì œì•ˆ: êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆ 3ê°€ì§€

ì´ 500ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
        "retention": """
[ë¶„ì„ ìš”ì²­: ë¦¬í…ì…˜ ë¶„ì„]
4ëª…ì˜ ì „ë¬¸ê°€(UA&ë¸Œëœë”© ë§ˆì¼€í„°, í¼ë¸”ë¦¬ì‹±, ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤, ë¼ì´ë¸Œ ì„œë¹„ìŠ¤)ê°€ ë¦¬í…ì…˜ ë° DAU íŒ¨í„´ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
1. DAU íŒ¨í„´ ê±´ê°•ë„: (ì¢‹ìŒ/ë³´í†µ/ìš°ë ¤ ì¤‘ í•˜ë‚˜ì™€ ì´ìœ )
2. í†µí•© ë¦¬í…ì…˜ ë¶„ì„: ë¦¬í…ì…˜ ì»¤ë¸Œ ë¶„ì„, ì¥ë¥´ ëŒ€ë¹„ ìˆ˜ì¤€, UA íš¨ìœ¨ ì˜í–¥ì„ í•˜ë‚˜ì˜ í†µí•©ëœ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„ì„ (ê° ì „ë¬¸ê°€ ì˜ê²¬ì„ ë‚˜ì—´í•˜ì§€ ë§ ê²ƒ)
3. ë¦¬í…ì…˜ ê°œì„  ì•¡ì…˜ í”Œëœ: ìš°ì„ ìˆœìœ„ë³„ 3ê°€ì§€

ì´ 400ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
        "revenue": """
[ë¶„ì„ ìš”ì²­: ë§¤ì¶œ ë¶„ì„]
4ëª…ì˜ ì „ë¬¸ê°€(UA&ë¸Œëœë”© ë§ˆì¼€í„°, í¼ë¸”ë¦¬ì‹±, ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤, ë¼ì´ë¸Œ ì„œë¹„ìŠ¤)ê°€ ë§¤ì¶œ ì˜ˆì¸¡ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
1. ë§¤ì¶œ ì˜ˆì¸¡ í˜„ì‹¤ì„±: (ë‚™ê´€ì /ì ì •/ë³´ìˆ˜ì  ì¤‘ í•˜ë‚˜ì™€ ì´ìœ )
2. í†µí•© ë§¤ì¶œ ë¶„ì„: ì†ìµë¶„ê¸°ì , ARPU, ê³¼ê¸ˆ ì „í™˜ìœ¨, ì‹œì¥ ì ìœ ìœ¨ì„ í•˜ë‚˜ì˜ í†µí•©ëœ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„ì„ (ê° ì „ë¬¸ê°€ ì˜ê²¬ì„ ë‚˜ì—´í•˜ì§€ ë§ ê²ƒ)
3. ë§¤ì¶œ ê·¹ëŒ€í™” ì „ëµ: ìš°ì„ ìˆœìœ„ë³„ 3ê°€ì§€

ì´ 400ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
        "risk": """
[ë¶„ì„ ìš”ì²­: ë¦¬ìŠ¤í¬ ë¶„ì„]
4ëª…ì˜ ì „ë¬¸ê°€(UA&ë¸Œëœë”© ë§ˆì¼€í„°, í¼ë¸”ë¦¬ì‹±, ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤, ë¼ì´ë¸Œ ì„œë¹„ìŠ¤)ê°€ ë¦¬ìŠ¤í¬ ìš”ì¸ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
1. ì „ì²´ ë¦¬ìŠ¤í¬ ìˆ˜ì¤€: (ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ ì¤‘ í•˜ë‚˜)
2. Best-Worst í¸ì°¨ ë¶„ì„: (í¸ì°¨ ë¹„ìœ¨ê³¼ ì˜ë¯¸)
3. í†µí•© ë¦¬ìŠ¤í¬ ë¶„ì„: ì¬ë¬´ ë¦¬ìŠ¤í¬, UA ë¦¬ìŠ¤í¬, ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„±, ì‹œì¥/ê²½ìŸ ë¦¬ìŠ¤í¬ë¥¼ í•˜ë‚˜ì˜ í†µí•©ëœ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„ì„ (ê° ì „ë¬¸ê°€ ì˜ê²¬ì„ ë‚˜ì—´í•˜ì§€ ë§ ê²ƒ)
4. ë¦¬ìŠ¤í¬ ì™„í™” ì „ëµ: ìš°ì„ ìˆœìœ„ë³„ 3ê°€ì§€

ì´ 450ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
        "competitive": """
[ë¶„ì„ ìš”ì²­: ê²½ìŸë ¥ ë¶„ì„]
4ëª…ì˜ ì „ë¬¸ê°€(UA&ë¸Œëœë”© ë§ˆì¼€í„°, í¼ë¸”ë¦¬ì‹±, ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤, ë¼ì´ë¸Œ ì„œë¹„ìŠ¤)ê°€ ì‹œì¥ ê²½ìŸë ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
1. ì‹œì¥ ê²½ìŸë ¥ ë“±ê¸‰: (ìƒ/ì¤‘/í•˜ ì¤‘ í•˜ë‚˜ì™€ ì´ìœ )
2. í†µí•© ê²½ìŸë ¥ ë¶„ì„: ì¥ë¥´ ë‚´ í¬ì§€ì…”ë‹, ì°¨ë³„í™” í¬ì¸íŠ¸, ìˆ˜ìµ ëª¨ë¸ ê²½ìŸë ¥ì„ í•˜ë‚˜ì˜ í†µí•©ëœ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„ì„ (ê° ì „ë¬¸ê°€ ì˜ê²¬ì„ ë‚˜ì—´í•˜ì§€ ë§ ê²ƒ)
3. ê²½ìŸë ¥ ê°•í™” ì „ëµ: ìš°ì„ ìˆœìœ„ë³„ 3ê°€ì§€

ì´ 400ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
    }
    
    return base_context + type_prompts.get(analysis_type, type_prompts["general"])

# API Endpoints
@app.get("/")
async def root():
    return {"message": "Game KPI Projection API", "version": "2.0.0", "ai_enabled": bool(CLAUDE_API_KEY)}

@app.get("/api/games")
async def get_available_games():
    raw_data = load_raw_data()
    return {
        "retention": list(raw_data['games']['retention'].keys()),
        "nru": list(raw_data['games']['nru'].keys()),
        "payment_rate": list(raw_data['games']['payment_rate'].keys()),
        "arppu": list(raw_data['games']['arppu'].keys())
    }

@app.get("/api/games/metadata")
async def get_games_metadata():
    """Get metadata for all games (release date, genre, platform, etc.)"""
    raw_data = load_raw_data()
    return raw_data.get('game_metadata', {})

@app.get("/api/games/{metric}/{game_name}")
async def get_game_data(metric: str, game_name: str):
    raw_data = load_raw_data()
    
    if metric not in raw_data['games']:
        raise HTTPException(status_code=404, detail=f"Metric '{metric}' not found")
    
    if game_name not in raw_data['games'][metric]:
        raise HTTPException(status_code=404, detail=f"Game '{game_name}' not found in {metric}")
    
    return {
        "game": game_name,
        "metric": metric,
        "data": raw_data['games'][metric][game_name]
    }

@app.get("/api/config")
async def get_default_config():
    return load_config()

@app.post("/api/projection")
async def calculate_projection(input_data: ProjectionInput):
    raw_data = load_raw_data()
    
    days = input_data.projection_days
    results = {"best": {}, "normal": {}, "worst": {}}
    
    # ============================================
    # V7: ë¸”ë Œë”© ì„¤ì • ì¶”ì¶œ
    # ============================================
    blending = input_data.blending or {}
    base_weight = blending.get("weight", 0.7)  # ê¸°ë³¸ê°’: ë‚´ë¶€ 70%
    genre = blending.get("genre", "MMORPG")
    platforms = blending.get("platforms", ["PC"])
    use_benchmark_only = blending.get("benchmark_only", False)
    use_time_decay = blending.get("time_decay", True)  # V7: Time-Decay ê¸°ë³¸ í™œì„±í™”
    
    # V7: Quality Score & BM Type
    quality_grade = input_data.quality_score or "B"
    quality_multiplier = QUALITY_SCORES.get(quality_grade, 1.0)
    bm_type = input_data.bm_type or "Midcore"
    bm_modifier = BM_TYPE_MODIFIERS.get(bm_type, {"pr_mod": 1.0, "arppu_mod": 1.0})
    
    # V7: ê³„ì ˆì„± íŒ©í„°
    regions = input_data.regions or ["global"]
    seasonality_factors = calculate_seasonality(regions, input_data.launch_date, days)
    
    # í‘œë³¸ ê²Œì„ì´ ì—†ìœ¼ë©´ ë²¤ì¹˜ë§ˆí¬ 100% ì‚¬ìš©
    has_sample_games = len(input_data.retention.selected_games) > 0
    if not has_sample_games:
        base_weight = 0.0
        use_benchmark_only = True
    
    # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (BM Type ì ìš©)
    benchmark = get_benchmark_data(genre, platforms)
    benchmark["pr"] = benchmark["pr"] * bm_modifier["pr_mod"]
    benchmark["arppu"] = benchmark["arppu"] * bm_modifier["arppu_mod"]
    benchmark_ret_curve = generate_benchmark_retention_curve(benchmark, days)
    
    # ë‚´ë¶€ í‘œë³¸ ê¸°ë°˜ ê³„ìˆ˜ ê³„ì‚°
    a, b = calculate_retention_coefficients(input_data.retention.selected_games, raw_data)
    pr_pattern = calculate_pr_pattern(input_data.revenue.selected_games_pr, raw_data)
    arppu_pattern = calculate_arppu_pattern(input_data.revenue.selected_games_arppu, raw_data)
    
    for scenario in ["best", "normal", "worst"]:
        target_d1 = input_data.retention.target_d1_retention[scenario]
        
        # ë‚´ë¶€ í‘œë³¸ ê¸°ë°˜ ë¦¬í…ì…˜ ì»¤ë¸Œ
        internal_ret_curve = generate_retention_curve(a, b, target_d1, days)
        
        # V7: Time-Decay ë¸”ë Œë”© ì ìš©
        if use_time_decay and not use_benchmark_only:
            # ë²¤ì¹˜ë§ˆí¬ ì»¤ë¸Œë¥¼ target_d1ì— ë§ê²Œ ìŠ¤ì¼€ì¼ë§
            benchmark_scale = target_d1 / benchmark["d1"] if benchmark["d1"] > 0 else 1.0
            scaled_benchmark_curve = [min(r * benchmark_scale, 1.0) for r in benchmark_ret_curve]
            ret_curve = calculate_time_decay_blended_retention(
                internal_ret_curve, scaled_benchmark_curve, days, quality_multiplier
            )
        elif not use_benchmark_only:
            # ê¸°ì¡´ ê³ ì • ë¸”ë Œë”©
            benchmark_scale = target_d1 / benchmark["d1"] if benchmark["d1"] > 0 else 1.0
            scaled_benchmark_curve = [min(r * benchmark_scale, 1.0) for r in benchmark_ret_curve]
            ret_curve = calculate_blended_retention(internal_ret_curve, scaled_benchmark_curve, base_weight)
        else:
            # ë²¤ì¹˜ë§ˆí¬ë§Œ ì‚¬ìš©
            benchmark_scale = target_d1 / benchmark["d1"] if benchmark["d1"] > 0 else 1.0
            ret_curve = [min(r * benchmark_scale * quality_multiplier, 1.0) for r in benchmark_ret_curve]
        
        # V7: NRU ì‹œë¦¬ì¦ˆ ìƒì„± (ëŸ°ì¹­ ë§ˆì¼€íŒ… D1~D30 ì§‘ì¤‘)
        d1_nru = input_data.nru.d1_nru[scenario]
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ NRU ë³´ì •
        nru_adj = input_data.nru.adjustment.get("best_vs_normal", 0) if scenario == "best" else \
                  input_data.nru.adjustment.get("worst_vs_normal", 0) if scenario == "worst" else 0
        adjusted_d1_nru = int(d1_nru * (1 + nru_adj))
        
        # V8.5: UA/Brand ë¶„ë¦¬ ì§€ì›
        ua_budget = input_data.nru.ua_budget or 0
        brand_budget = input_data.nru.brand_budget or 0
        target_cpa = input_data.nru.target_cpa or 2000
        base_organic_ratio = input_data.nru.base_organic_ratio or 0.2
        
        # UA/Brand ì˜ˆì‚°ì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ V8.5 ë¡œì§ ì‚¬ìš©
        if ua_budget > 0:
            # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì˜ˆì‚° ì¡°ì •
            scenario_mult = 1.1 if scenario == "best" else (0.9 if scenario == "worst" else 1.0)
            adj_ua = int(ua_budget * scenario_mult)
            adj_brand = int(brand_budget * scenario_mult)
            
            sustaining_monthly = input_data.basic_settings.get("sustaining_mkt_budget_monthly", 0) if input_data.basic_settings else 0
            
            # V8.5+ ì‹ ê·œ íŒŒë¼ë¯¸í„°
            pre_marketing_ratio = input_data.nru.pre_marketing_ratio or 0.0
            wishlist_conversion_rate = input_data.nru.wishlist_conversion_rate or 0.15
            cpa_saturation_enabled = input_data.nru.cpa_saturation_enabled if input_data.nru.cpa_saturation_enabled is not None else True
            brand_time_lag_enabled = input_data.nru.brand_time_lag_enabled if input_data.nru.brand_time_lag_enabled is not None else True
            
            nru_series, paid_nru, organic_nru, organic_boost, nru_meta = generate_nru_series_v85(
                adj_ua, adj_brand, target_cpa, base_organic_ratio, days, 30, sustaining_monthly,
                pre_marketing_ratio, wishlist_conversion_rate, cpa_saturation_enabled, brand_time_lag_enabled
            )
            
            # ì‹œë‚˜ë¦¬ì˜¤ë³„ ë©”íƒ€ ì •ë³´ ì €ì¥
            if scenario == "normal":
                v85_nru_meta = {
                    "paid_nru": paid_nru,
                    "organic_nru": organic_nru,
                    "organic_boost_factor": round(organic_boost, 2),
                    "total_nru": paid_nru + organic_nru,
                    # V8.5+ ì¶”ê°€ ë©”íƒ€
                    "effective_cpa": nru_meta["effective_cpa"],
                    "cpa_saturation_factor": nru_meta["cpa_saturation_factor"],
                    "pre_launch_users": nru_meta["pre_launch_users"],
                    "wishlist_users": nru_meta["wishlist_users"],
                    "d1_burst_users": nru_meta["d1_burst_users"],
                    "brand_time_lag_peak_day": nru_meta["brand_time_lag_peak_day"]
                }
        else:
            # ê¸°ì¡´ ë¡œì§ (d1_nru ì§ì ‘ ì…ë ¥)
            nru_series = generate_nru_series(adjusted_d1_nru, [], days)
            v85_nru_meta = None
        
        # V7: ê³„ì ˆì„± ì ìš© (NRUì— ë°˜ì˜)
        nru_series = [int(nru * sf) for nru, sf in zip(nru_series, seasonality_factors)]
        
        # DAU ê³„ì‚°
        dau_series = calculate_dau_matrix(nru_series, ret_curve, days)
        
        # PR ë³´ì •
        pr_adj = input_data.revenue.pr_adjustment.get("best_vs_normal", 0) if scenario == "best" else \
                 input_data.revenue.pr_adjustment.get("worst_vs_normal", 0) if scenario == "worst" else 0
        
        # PR ë¸”ë Œë”© (BM Type ì ìš©ë¨) + V7: Quality Scoreë„ ì ìš©
        if not use_benchmark_only:
            weight_internal = base_weight
            # ë²¤ì¹˜ë§ˆí¬ PRì— Quality Score ì ìš©
            adjusted_benchmark_pr = benchmark["pr"] * quality_multiplier
            pr_series = calculate_blended_pr(pr_pattern, adjusted_benchmark_pr, weight_internal, days)
        else:
            # ë²¤ì¹˜ë§ˆí¬ë§Œ ì‚¬ìš© ì‹œì—ë„ Quality Score ì ìš©
            pr_series = [benchmark["pr"] * quality_multiplier] * days
        pr_series = [p * (1 + pr_adj) for p in pr_series]
        
        # ARPPU ë³´ì •
        arppu_adj = input_data.revenue.arppu_adjustment.get("best_vs_normal", 0) if scenario == "best" else \
                    input_data.revenue.arppu_adjustment.get("worst_vs_normal", 0) if scenario == "worst" else 0
        
        # ARPPU ë¸”ë Œë”© (BM Type ì ìš©ë¨) + V7: Quality Scoreë„ ì ìš©
        if not use_benchmark_only:
            # ë²¤ì¹˜ë§ˆí¬ ARPPUì— Quality Score ì ìš©
            adjusted_benchmark_arppu = benchmark["arppu"] * quality_multiplier
            arppu_series = calculate_blended_arppu(arppu_pattern, adjusted_benchmark_arppu, base_weight, days)
        else:
            # ë²¤ì¹˜ë§ˆí¬ë§Œ ì‚¬ìš© ì‹œì—ë„ Quality Score ì ìš©
            arppu_series = [benchmark["arppu"] * quality_multiplier] * days
        arppu_series = [a * (1 + arppu_adj) for a in arppu_series]
        
        # V7: ê³„ì ˆì„±ì„ ARPPUì—ë„ ë°˜ì˜
        arppu_series = [arppu * sf for arppu, sf in zip(arppu_series, seasonality_factors)]
        
        # Revenue ê³„ì‚° (ì¼ë³„ ARPPU í™˜ì‚° ì ìš©ë¨)
        revenue_series = calculate_revenue(dau_series, pr_series, arppu_series)
        
        results[scenario] = {
            "retention": {
                "coefficients": {"a": float(a), "b": float(b)},
                "target_d1": target_d1,
                "curve": ret_curve[:90]
            },
            "nru": {
                "d1_nru": d1_nru,
                "series": nru_series[:90],
                "total": sum(nru_series),
                "paid": paid_nru if ua_budget > 0 else sum(nru_series),
                "organic": organic_nru if ua_budget > 0 else 0
            },
            "dau": {
                "series": dau_series[:90],
                "peak": int(max(dau_series)),
                "average": int(np.mean(dau_series))
            },
            "revenue": {
                "pr_series": pr_series[:90],
                "arppu_series": arppu_series[:90],
                "daily_revenue": revenue_series[:90],
                "total_gross": sum(revenue_series),
                "average_daily": float(np.mean(revenue_series))
            },
            "full_data": {
                "nru": nru_series,
                "dau": dau_series,
                "revenue": revenue_series,
                "retention": ret_curve,
                "pr": pr_series,
                "arppu": arppu_series
            }
        }
    
    # Calculate summary
    summary = {}
    
    # V8.5: ë§ˆì¼€íŒ… ì˜ˆì‚° ì´í•© ê³„ì‚°
    ua_budget = input_data.nru.ua_budget or 0
    brand_budget = input_data.nru.brand_budget or 0
    basic = input_data.basic_settings or load_config()["basic_settings"]
    sustaining_monthly = basic.get("sustaining_mkt_budget_monthly", 0)
    total_sustaining = sustaining_monthly * 12  # ì—°ê°„ ìœ ì§€ ì˜ˆì‚°
    
    total_marketing_budget = ua_budget + brand_budget + total_sustaining
    
    for scenario in ["best", "normal", "worst"]:
        gross = results[scenario]["revenue"]["total_gross"]
        
        market_fee = basic.get("market_fee_ratio", 0.3)
        vat = basic.get("vat_ratio", 0.1)
        infra = basic.get("infrastructure_cost_ratio", 0.03)
        
        net = gross * (1 - market_fee - vat - infra)
        
        # V8.5: ROAS ê³„ì‚° ë¶„ë¦¬
        # Paid ROAS: í¼í¬ë¨¼ìŠ¤ ë§ˆì¼€íŒ…(UA) íš¨ìœ¨ (ë§ˆì¼€í„°ìš©)
        paid_roas = (gross / ua_budget * 100) if ua_budget > 0 else 0
        
        # Blended ROAS: ì „ì²´ ë§ˆì¼€íŒ… íš¨ìœ¨ (ê²½ì˜ì§„ ë³´ê³ ìš©)
        blended_roas = (gross / total_marketing_budget * 100) if total_marketing_budget > 0 else 0
        
        # LTV, CAC ê³„ì‚°
        total_nru = results[scenario]["nru"]["total"]
        paid_nru_count = results[scenario]["nru"].get("paid", total_nru)
        
        ltv = gross / total_nru if total_nru > 0 else 0
        cac_paid = ua_budget / paid_nru_count if paid_nru_count > 0 else 0
        cac_blended = total_marketing_budget / total_nru if total_nru > 0 else 0
        
        summary[scenario] = {
            "gross_revenue": gross,
            "net_revenue": net,
            "total_nru": total_nru,
            "peak_dau": results[scenario]["dau"]["peak"],
            "average_dau": results[scenario]["dau"]["average"],
            "average_daily_revenue": results[scenario]["revenue"]["average_daily"],
            # V8.5: ROAS ë¶„ë¦¬
            "paid_roas": round(paid_roas, 1),      # UA íš¨ìœ¨ (ë§ˆì¼€í„°ìš©)
            "blended_roas": round(blended_roas, 1), # ì „ì²´ íš¨ìœ¨ (ê²½ì˜ì§„ìš©)
            "ltv": round(ltv, 0),
            "cac_paid": round(cac_paid, 0),
            "cac_blended": round(cac_blended, 0)
        }
    
    # V8.5: ë§ˆì¼€íŒ… ì˜ˆì‚° ë¶„ì„ ì •ë³´
    v85_marketing_analysis = {
        "ua_budget": ua_budget,
        "brand_budget": brand_budget,
        "sustaining_budget_annual": total_sustaining,
        "total_marketing_budget": total_marketing_budget,
        "organic_boost_factor": round(calculate_organic_boost(brand_budget, ua_budget), 2) if ua_budget > 0 else 1.0,
        "budget_breakdown": {
            "ua_ratio": round(ua_budget / total_marketing_budget * 100, 1) if total_marketing_budget > 0 else 0,
            "brand_ratio": round(brand_budget / total_marketing_budget * 100, 1) if total_marketing_budget > 0 else 0,
            "sustaining_ratio": round(total_sustaining / total_marketing_budget * 100, 1) if total_marketing_budget > 0 else 0
        },
        # V8.5+ ì‹ ê·œ ë©”íƒ€ ì •ë³´
        "pre_launch_settings": {
            "pre_marketing_ratio": input_data.nru.pre_marketing_ratio or 0.0,
            "wishlist_conversion_rate": input_data.nru.wishlist_conversion_rate or 0.15,
            "cpa_saturation_enabled": input_data.nru.cpa_saturation_enabled if input_data.nru.cpa_saturation_enabled is not None else True,
            "brand_time_lag_enabled": input_data.nru.brand_time_lag_enabled if input_data.nru.brand_time_lag_enabled is not None else True
        },
        "nru_analysis": v85_nru_meta if 'v85_nru_meta' in dir() and v85_nru_meta else None
    }
    
    return {
        "status": "success",
        "input": {
            "launch_date": input_data.launch_date,
            "projection_days": days,
            "retention_games": input_data.retention.selected_games,
            "nru_games": input_data.nru.selected_games,
            "pr_games": input_data.revenue.selected_games_pr,
            "arppu_games": input_data.revenue.selected_games_arppu
        },
        "blending": {
            "weight_internal": base_weight,
            "weight_benchmark": 1 - base_weight,
            "time_decay": use_time_decay,
            "genre": genre,
            "platforms": platforms,
            "benchmark_only": use_benchmark_only,
            "benchmark_data": benchmark
        },
        "v7_settings": {
            "quality_score": quality_grade,
            "quality_multiplier": quality_multiplier,
            "bm_type": bm_type,
            "bm_modifier": bm_modifier,
            "regions": regions,
            "seasonality_applied": True
        },
        "v85_marketing": v85_marketing_analysis,  # V8.5: ë§ˆì¼€íŒ… ë¶„ì„ ì¶”ê°€
        "summary": summary,
        "results": results
    }

# V9.8: Mock AI Report Generator (Fallbackìš©)
def generate_mock_ai_report(summary: Dict[str, Any], analysis_type: str) -> str:
    """API ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  Mock ë³´ê³ ì„œ ìƒì„±"""
    genre = summary.get('blending', {}).get('genre', 'N/A')
    platforms = ', '.join(summary.get('blending', {}).get('platforms', ['PC']))
    normal_revenue = summary.get('normal', {}).get('gross_revenue', 0)
    bep_day = summary.get('bep_day', -1)
    
    bep_status = f"D+{bep_day}ì— BEP ë‹¬ì„± ì˜ˆìƒ" if bep_day > 0 else "1ë…„ ë‚´ BEP ë¯¸ë‹¬ì„± ìœ„í—˜"
    
    if analysis_type == "executive_report":
        return f"""[ì¢…í•© ë¶„ì„ ìš”ì•½]
{genre} ì¥ë¥´ì˜ {platforms} í”Œë«í¼ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. 
Normal ì‹œë‚˜ë¦¬ì˜¤ ê¸°ì¤€ ì´ ë§¤ì¶œ {normal_revenue:,.0f}ì›ì´ ì˜ˆìƒë©ë‹ˆë‹¤.
{bep_status}ì…ë‹ˆë‹¤.

[í•µì‹¬ ì§€í‘œ í‰ê°€]
1. ë§¤ì¶œ ì „ë§: Normal ì‹œë‚˜ë¦¬ì˜¤ ê¸°ì¤€ ì ì • ìˆ˜ì¤€
2. ë¦¬í…ì…˜: ì¥ë¥´ í‰ê·  ëŒ€ë¹„ ê²€í†  í•„ìš”
3. ë§ˆì¼€íŒ… íš¨ìœ¨: CPA/CPI ìµœì í™” ì—¬ì§€ ì¡´ì¬

[ë¦¬ìŠ¤í¬ ë¶„ì„]
1. ì‹œì¥ ê²½ìŸ: ë™ì¼ ì¥ë¥´ ì¶œì‹œì‘ ëª¨ë‹ˆí„°ë§ í•„ìš”
2. ìœ ì € í™•ë³´: ëŸ°ì¹­ ì´ˆê¸° ì§‘ì¤‘ ë§ˆì¼€íŒ… ê¶Œì¥
3. ìˆ˜ìµí™”: BM ëª¨ë¸ ìµœì í™” ê²€í† 

[ì „ëµ ì œì–¸]
1. ëŸ°ì¹­ ì „ ì‚¬ì „ ë§ˆì¼€íŒ…ìœ¼ë¡œ ìœ„ì‹œë¦¬ìŠ¤íŠ¸ í™•ë³´
2. D1 ë¦¬í…ì…˜ í™•ë³´ë¥¼ ìœ„í•œ ì˜¨ë³´ë”© ìµœì í™”
3. ë¼ì´ë¸Œ ì„œë¹„ìŠ¤ ì¤€ë¹„ë¡œ ì¥ê¸° ìš´ì˜ ëŒ€ë¹„

* ì´ ë³´ê³ ì„œëŠ” AI ì—°ê²° ì‹¤íŒ¨ë¡œ ì¸í•œ ê¸°ë³¸ ë¶„ì„ì…ë‹ˆë‹¤."""
    else:
        return f"[{analysis_type}] {genre} í”„ë¡œì íŠ¸ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ìƒì„¸ AI ë¶„ì„ì„ ìœ„í•´ API ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

# AI Insight Endpoint
@app.post("/api/ai/insight")
async def get_ai_insight_endpoint(request: AIInsightRequest):
    """Get AI-powered insights for projection results with Mock Fallback"""
    prompt = create_insight_prompt(request.projection_summary, request.analysis_type)
    insight = await get_claude_insight(prompt)
    
    # V9.8: Mock Fallback
    if insight is None:
        print("âš ï¸ AI API failed. Using Mock Report.")
        insight = generate_mock_ai_report(request.projection_summary, request.analysis_type)
        ai_model = "mock-fallback"
    else:
        ai_model = CURRENT_MODEL
    
    return {
        "status": "success",
        "analysis_type": request.analysis_type,
        "insight": insight,
        "ai_model": ai_model
    }

@app.get("/api/ai/status")
async def get_ai_status():
    """Check AI integration status"""
    return {
        "enabled": bool(CLAUDE_API_KEY),
        "model": CURRENT_MODEL,
        "available_types": ["general", "reliability", "retention", "revenue", "risk", "competitive"]
    }

@app.get("/api/raw-data")
async def get_raw_data():
    return load_raw_data()

@app.get("/api/raw-data/download")
async def download_raw_data_excel():
    """Download raw game data as Excel file (same format as original)"""
    import io
    from openpyxl import Workbook
    from openpyxl.styles import Font, PatternFill, Border, Side, Alignment
    from fastapi.responses import StreamingResponse
    
    raw_data = load_raw_data()
    wb = Workbook()
    
    # ìŠ¤íƒ€ì¼ ì •ì˜
    header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
    header_font = Font(color="FFFFFF", bold=True)
    thin_border = Border(
        left=Side(style='thin'),
        right=Side(style='thin'),
        top=Side(style='thin'),
        bottom=Side(style='thin')
    )
    
    def create_raw_sheet(ws, sheet_title, metric_name, description, data_dict):
        """Raw ë°ì´í„° ì‹œíŠ¸ ìƒì„± (ì›ë³¸ ì—‘ì…€ í˜•ì‹)"""
        # Row 1: ì•ˆë‚´ ë¬¸êµ¬
        ws['B1'] = f'- ì•„ë˜ ê²Œì„ ì¶”ê°€ ì‹œ {sheet_title} ê²Œì„ ë¦¬ìŠ¤íŠ¸ì— ìë™ìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤.'
        ws['B1'].font = Font(color="FF0000")
        
        # Row 2: ë©”íŠ¸ë¦­ëª… ë° ì„¤ëª…
        ws['B2'] = metric_name
        ws['B2'].font = Font(bold=True)
        ws['C2'] = description
        
        # Row 3: í—¤ë” (ê²Œì„ëª…, 1, 2, 3, ... 365)
        ws['B3'] = 'ê²Œì„ëª…'
        ws['B3'].fill = header_fill
        ws['B3'].font = header_font
        ws['B3'].border = thin_border
        
        max_days = 90 if metric_name == 'ë¦¬í…ì…˜' else 365
        for day in range(1, max_days + 1):
            col = day + 2  # Cë¶€í„° ì‹œì‘
            cell = ws.cell(row=3, column=col, value=day)
            cell.fill = header_fill
            cell.font = header_font
            cell.border = thin_border
        
        # Row 4+: ê²Œì„ ë°ì´í„°
        row_idx = 4
        for game_name, values in data_dict.items():
            ws.cell(row=row_idx, column=2, value=game_name).border = thin_border
            for i, val in enumerate(values[:max_days]):
                cell = ws.cell(row=row_idx, column=i + 3, value=val)
                cell.border = thin_border
                if metric_name in ['ë¦¬í…ì…˜', 'PR']:
                    cell.number_format = '0.00%'
            row_idx += 1
        
        # ì—´ ë„ˆë¹„ ì¡°ì •
        ws.column_dimensions['B'].width = 20
        for col in range(3, max_days + 3):
            ws.column_dimensions[ws.cell(row=3, column=col).column_letter].width = 8
    
    # Raw_Retention ì‹œíŠ¸
    ws_retention = wb.active
    ws_retention.title = "Raw_Retention"
    create_raw_sheet(ws_retention, "1. Retention", "ë¦¬í…ì…˜", "ë¡ ì¹­ ~ 90ì¼ê¹Œì§€ì˜ ë¦¬í…ì…˜ ì •ë³´ ì…ë ¥", raw_data['games'].get('retention', {}))
    
    # Raw_NRU ì‹œíŠ¸
    ws_nru = wb.create_sheet("Raw_NRU")
    create_raw_sheet(ws_nru, "2. NRU", "NRU", "ë¡ ì¹­ ~ 365ì¼ê¹Œì§€ì˜ ë°ì´í„° ì…ë ¥", raw_data['games'].get('nru', {}))
    
    # Raw_PR ì‹œíŠ¸
    ws_pr = wb.create_sheet("Raw_PR")
    create_raw_sheet(ws_pr, "3. Revenue", "PR", "ë¡ ì¹­ ~ 365ì¼ê¹Œì§€ì˜ ë°ì´í„° ì…ë ¥", raw_data['games'].get('payment_rate', {}))
    
    # Raw_ARPPU ì‹œíŠ¸
    ws_arppu = wb.create_sheet("Raw_ARPPU")
    create_raw_sheet(ws_arppu, "3. Revenue", "ARPPU", "ë¡ ì¹­ ~ 365ì¼ê¹Œì§€ì˜ ë°ì´í„° ì…ë ¥", raw_data['games'].get('arppu', {}))
    
    # ë©”ëª¨ë¦¬ì— ì €ì¥
    output = io.BytesIO()
    wb.save(output)
    output.seek(0)
    
    return StreamingResponse(
        output,
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        headers={"Content-Disposition": "attachment; filename=raw_game_data.xlsx"}
    )

@app.post("/api/raw-data/upload")
async def upload_game_data(file: UploadFile = File(...), metric: str = "retention"):
    if not file.filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="Only CSV files are supported")
    
    import pandas as pd
    from io import StringIO
    
    content = await file.read()
    df = pd.read_csv(StringIO(content.decode('utf-8')))
    
    raw_data = load_raw_data()
    
    for _, row in df.iterrows():
        game_name = row.iloc[0]
        values = row.iloc[1:].tolist()
        values = [float(v) for v in values if pd.notna(v)]
        
        if metric in raw_data['games']:
            raw_data['games'][metric][game_name] = values
    
    raw_data['metadata'][f'{metric}_games'] = list(raw_data['games'][metric].keys())
    
    with open(RAW_DATA_PATH, 'w', encoding='utf-8') as f:
        json.dump(raw_data, f, ensure_ascii=False, indent=2)
    
    return {"status": "success", "message": f"Added/updated games in {metric}"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
