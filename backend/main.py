from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import numpy as np
from scipy.optimize import curve_fit
import json
import os
import httpx

app = FastAPI(title="Game KPI Projection API", version="2.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Data paths
DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
RAW_DATA_PATH = os.path.join(DATA_DIR, "raw_game_data.json")
CONFIG_PATH = os.path.join(DATA_DIR, "default_config.json")

# Claude API Configuration
CLAUDE_API_KEY = os.environ.get("CLAUDE_API_KEY", "")
CLAUDE_API_URL = "https://api.anthropic.com/v1/messages"

def load_raw_data():
    with open(RAW_DATA_PATH, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_config():
    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
        return json.load(f)

# Pydantic Models
class RetentionInput(BaseModel):
    selected_games: List[str]
    target_d1_retention: Dict[str, float]

class NRUInput(BaseModel):
    selected_games: List[str]
    d1_nru: Dict[str, int]
    paid_organic_ratio: float
    nvr: float
    adjustment: Dict[str, float]

class RevenueInput(BaseModel):
    selected_games_pr: List[str]
    selected_games_arppu: List[str]
    pr_adjustment: Dict[str, float]
    arppu_adjustment: Dict[str, float]

class ProjectionInput(BaseModel):
    launch_date: str
    projection_days: int = 365
    retention: RetentionInput
    nru: NRUInput
    revenue: RevenueInput
    basic_settings: Optional[Dict[str, Any]] = None
    # ë¸”ë Œë”© ì„¤ì •
    blending: Optional[Dict[str, Any]] = None  # { weight: 0.7, genre: "MMORPG", platforms: ["PC"] }
    # V7 ì¶”ê°€: í’ˆì§ˆ ì ìˆ˜, BM íƒ€ì…, ì§€ì—­
    quality_score: Optional[str] = "B"  # S/A/B/C/D
    bm_type: Optional[str] = "Midcore"  # Hardcore/Midcore/Casual/F2P_Cosmetic/Gacha
    regions: Optional[List[str]] = None  # ["korea", "japan", "global", ...]

# ============================================
# ê¸€ë¡œë²Œ ê³„ì ˆì„± íŒ©í„° (ì§€ì—­ë³„ ì›”ê°„ ê°€ì¤‘ì¹˜)
# ============================================
SEASONALITY_BY_REGION = {
    "korea": {1: 1.15, 2: 1.20, 3: 1.00, 4: 0.95, 5: 1.00, 6: 0.95, 7: 1.05, 8: 1.10, 9: 1.00, 10: 1.05, 11: 1.10, 12: 1.15},
    "japan": {1: 1.10, 2: 1.05, 3: 1.05, 4: 1.10, 5: 1.15, 6: 0.95, 7: 1.00, 8: 1.05, 9: 1.00, 10: 1.00, 11: 1.05, 12: 1.20},
    "china": {1: 1.10, 2: 1.25, 3: 1.00, 4: 0.95, 5: 1.05, 6: 1.10, 7: 1.05, 8: 1.00, 9: 1.00, 10: 1.20, 11: 1.15, 12: 1.05},
    "global": {1: 0.95, 2: 0.90, 3: 0.95, 4: 1.00, 5: 1.00, 6: 1.00, 7: 1.05, 8: 1.00, 9: 1.00, 10: 1.05, 11: 1.15, 12: 1.25},
    "sea": {1: 1.05, 2: 1.10, 3: 1.00, 4: 1.00, 5: 1.00, 6: 1.05, 7: 1.05, 8: 1.00, 9: 1.00, 10: 1.00, 11: 1.05, 12: 1.15},
    "na": {1: 0.90, 2: 0.90, 3: 0.95, 4: 1.00, 5: 1.00, 6: 1.05, 7: 1.05, 8: 1.00, 9: 0.95, 10: 1.05, 11: 1.20, 12: 1.25},
    "sa": {1: 1.10, 2: 1.05, 3: 1.00, 4: 0.95, 5: 0.95, 6: 1.00, 7: 1.05, 8: 1.00, 9: 1.00, 10: 1.05, 11: 1.10, 12: 1.15},
    "eu": {1: 0.90, 2: 0.90, 3: 0.95, 4: 1.05, 5: 1.00, 6: 1.00, 7: 1.00, 8: 0.95, 9: 1.00, 10: 1.05, 11: 1.15, 12: 1.25},
}

def calculate_seasonality(regions: List[str], launch_date: str, days: int = 365) -> List[float]:
    """
    ì§€ì—­ë³„ ê³„ì ˆì„± íŒ©í„° ê³„ì‚°
    ë‹¤ì¤‘ ì§€ì—­ ì„ íƒ ì‹œ í‰ê·  ì ìš©
    """
    from datetime import datetime, timedelta
    
    try:
        start_date = datetime.strptime(launch_date, "%Y-%m-%d")
    except:
        start_date = datetime(2026, 11, 12)  # ê¸°ë³¸ê°’
    
    factors = []
    for day in range(days):
        current_date = start_date + timedelta(days=day)
        month = current_date.month
        
        # ì„ íƒëœ ì§€ì—­ë“¤ì˜ ê³„ì ˆì„± í‰ê· 
        region_factors = []
        for region in regions:
            region_key = region.lower()
            if region_key in SEASONALITY_BY_REGION:
                region_factors.append(SEASONALITY_BY_REGION[region_key].get(month, 1.0))
        
        if region_factors:
            factors.append(np.mean(region_factors))
        else:
            factors.append(1.0)
    
    return factors

# ============================================
# Time-Decay ë¸”ë Œë”© (ì‹œê°„ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë³€ê²½)
# ============================================
def calculate_time_decay_weight(day: int, days: int = 365) -> float:
    """
    ì‹œê°„ ê°€ì¤‘ì¹˜ ê³„ì‚° (Time-Decay)
    
    D1: ë‚´ë¶€ 90% : ë²¤ì¹˜ë§ˆí¬ 10%
    D180: ë‚´ë¶€ 50% : ë²¤ì¹˜ë§ˆí¬ 50%
    D365: ë‚´ë¶€ 10% : ë²¤ì¹˜ë§ˆí¬ 90%
    
    ì„ í˜• ë³´ê°„ìœ¼ë¡œ ë§¤ì¼ ê°€ì¤‘ì¹˜ ë³€ê²½
    """
    # D1 = 0.9, D365 = 0.1 (ì„ í˜• ê°ì†Œ)
    weight_internal = 0.9 - (0.8 * (day - 1) / (days - 1)) if days > 1 else 0.9
    return max(min(weight_internal, 0.9), 0.1)

def calculate_time_decay_blended_retention(
    internal_curve: List[float],
    benchmark_curve: List[float],
    days: int = 365,
    quality_score: float = 1.0
) -> List[float]:
    """
    Time-Decay ë¸”ë Œë”© ë¦¬í…ì…˜ ì»¤ë¸Œ ìƒì„±
    
    Args:
        internal_curve: ë‚´ë¶€ í‘œë³¸ ë¦¬í…ì…˜ ì»¤ë¸Œ
        benchmark_curve: ë²¤ì¹˜ë§ˆí¬ ë¦¬í…ì…˜ ì»¤ë¸Œ
        days: í”„ë¡œì ì…˜ ê¸°ê°„
        quality_score: í’ˆì§ˆ ì ìˆ˜ (S=1.2, A=1.1, B=1.0, C=0.9, D=0.8)
    """
    blended = []
    for day in range(days):
        weight_internal = calculate_time_decay_weight(day + 1, days)
        weight_benchmark = 1 - weight_internal
        
        internal_val = internal_curve[day] if day < len(internal_curve) else internal_curve[-1]
        benchmark_val = benchmark_curve[day] if day < len(benchmark_curve) else benchmark_curve[-1]
        
        # ë²¤ì¹˜ë§ˆí¬ì— í’ˆì§ˆ ì ìˆ˜ ì ìš©
        adjusted_benchmark = benchmark_val * quality_score
        
        blended_val = (internal_val * weight_internal) + (adjusted_benchmark * weight_benchmark)
        blended.append(max(min(blended_val, 1.0), 0.001))
    
    return blended

# ============================================
# Quality Score ì •ì˜
# ============================================
QUALITY_SCORES = {
    "S": 1.2,   # ìµœìƒê¸‰ (FGT/CBT ê²°ê³¼ ë§¤ìš° ìš°ìˆ˜)
    "A": 1.1,   # ìš°ìˆ˜
    "B": 1.0,   # ë³´í†µ (ê¸°ë³¸ê°’)
    "C": 0.9,   # ë¯¸í¡
    "D": 0.8,   # ë¶€ì§„
}

# ============================================
# BM íƒ€ì…ë³„ ì„¸ë¶„í™” ë²¤ì¹˜ë§ˆí¬
# ============================================
BM_TYPE_MODIFIERS = {
    "Hardcore": {"pr_mod": 0.5, "arppu_mod": 2.0},    # ë‚®ì€ PR, ê³ ì•¡ ARPPU
    "Midcore": {"pr_mod": 1.0, "arppu_mod": 1.0},     # ê¸°ë³¸
    "Casual": {"pr_mod": 2.0, "arppu_mod": 0.4},      # ë†’ì€ PR, ì†Œì•¡ ARPPU
    "F2P_Cosmetic": {"pr_mod": 0.8, "arppu_mod": 0.6}, # ë¬´ë£Œ+ê¾¸ë¯¸ê¸° ì¤‘ì‹¬
    "Gacha": {"pr_mod": 1.5, "arppu_mod": 1.8},       # ê°€ì±  ì¤‘ì‹¬
}
BENCHMARK_DATA = {
    "PC": {
        "MMORPG": {"d1": 0.32, "d7": 0.20, "d30": 0.11, "d90": 0.06, "pr": 0.06, "arppu": 78000},
        "Action RPG": {"d1": 0.30, "d7": 0.18, "d30": 0.09, "d90": 0.04, "pr": 0.05, "arppu": 65000},
        "Battle Royale": {"d1": 0.35, "d7": 0.22, "d30": 0.12, "d90": 0.07, "pr": 0.03, "arppu": 45000},
        "Extraction Shooter": {"d1": 0.28, "d7": 0.16, "d30": 0.08, "d90": 0.04, "pr": 0.04, "arppu": 55000},
        "FPS/TPS": {"d1": 0.33, "d7": 0.20, "d30": 0.10, "d90": 0.05, "pr": 0.04, "arppu": 50000},
        "Strategy": {"d1": 0.25, "d7": 0.15, "d30": 0.08, "d90": 0.04, "pr": 0.07, "arppu": 85000},
        "Casual": {"d1": 0.40, "d7": 0.20, "d30": 0.08, "d90": 0.03, "pr": 0.02, "arppu": 25000},
        "Sports": {"d1": 0.30, "d7": 0.18, "d30": 0.09, "d90": 0.04, "pr": 0.05, "arppu": 60000},
    },
    "Mobile": {
        "MMORPG": {"d1": 0.42, "d7": 0.18, "d30": 0.07, "d90": 0.03, "pr": 0.05, "arppu": 52000},
        "Action RPG": {"d1": 0.38, "d7": 0.15, "d30": 0.06, "d90": 0.02, "pr": 0.04, "arppu": 45000},
        "Battle Royale": {"d1": 0.45, "d7": 0.20, "d30": 0.08, "d90": 0.04, "pr": 0.02, "arppu": 35000},
        "Extraction Shooter": {"d1": 0.35, "d7": 0.14, "d30": 0.05, "d90": 0.02, "pr": 0.03, "arppu": 40000},
        "FPS/TPS": {"d1": 0.40, "d7": 0.17, "d30": 0.07, "d90": 0.03, "pr": 0.03, "arppu": 38000},
        "Strategy": {"d1": 0.35, "d7": 0.16, "d30": 0.07, "d90": 0.03, "pr": 0.06, "arppu": 68000},
        "Casual": {"d1": 0.50, "d7": 0.22, "d30": 0.09, "d90": 0.04, "pr": 0.02, "arppu": 18000},
        "Sports": {"d1": 0.38, "d7": 0.16, "d30": 0.06, "d90": 0.02, "pr": 0.04, "arppu": 42000},
    },
    "Console": {
        "MMORPG": {"d1": 0.35, "d7": 0.22, "d30": 0.12, "d90": 0.06, "pr": 0.05, "arppu": 70000},
        "Action RPG": {"d1": 0.33, "d7": 0.20, "d30": 0.10, "d90": 0.05, "pr": 0.04, "arppu": 60000},
        "Battle Royale": {"d1": 0.38, "d7": 0.24, "d30": 0.13, "d90": 0.07, "pr": 0.02, "arppu": 40000},
        "Extraction Shooter": {"d1": 0.30, "d7": 0.18, "d30": 0.09, "d90": 0.04, "pr": 0.03, "arppu": 50000},
        "FPS/TPS": {"d1": 0.36, "d7": 0.22, "d30": 0.11, "d90": 0.06, "pr": 0.03, "arppu": 48000},
        "Strategy": {"d1": 0.28, "d7": 0.17, "d30": 0.09, "d90": 0.04, "pr": 0.06, "arppu": 75000},
        "Casual": {"d1": 0.42, "d7": 0.20, "d30": 0.08, "d90": 0.03, "pr": 0.02, "arppu": 22000},
        "Sports": {"d1": 0.35, "d7": 0.20, "d30": 0.10, "d90": 0.05, "pr": 0.05, "arppu": 55000},
    }
}

def get_benchmark_data(genre: str, platforms: List[str]) -> Dict[str, float]:
    """ì¥ë¥´/í”Œë«í¼ì— ë§ëŠ” ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë°˜í™˜ (ë‹¤ì¤‘ í”Œë«í¼ì€ í‰ê· )"""
    if not platforms:
        platforms = ["PC"]
    
    values = []
    for platform in platforms:
        if platform in BENCHMARK_DATA and genre in BENCHMARK_DATA[platform]:
            values.append(BENCHMARK_DATA[platform][genre])
    
    if not values:
        # ê¸°ë³¸ê°’ (PC/MMORPG)
        return {"d1": 0.32, "d7": 0.20, "d30": 0.11, "d90": 0.06, "pr": 0.06, "arppu": 78000}
    
    # ë‹¤ì¤‘ í”Œë«í¼ì´ë©´ í‰ê· 
    return {
        "d1": np.mean([v["d1"] for v in values]),
        "d7": np.mean([v["d7"] for v in values]),
        "d30": np.mean([v["d30"] for v in values]),
        "d90": np.mean([v["d90"] for v in values]),
        "pr": np.mean([v["pr"] for v in values]),
        "arppu": np.mean([v["arppu"] for v in values]),
    }

def generate_benchmark_retention_curve(benchmark: Dict[str, float], days: int = 365) -> List[float]:
    """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë¡œ Power Law ë¦¬í…ì…˜ ì»¤ë¸Œ ìƒì„±"""
    # D1, D7, D30, D90 ë°ì´í„°ë¡œ íšŒê·€ë¶„ì„
    x_data = np.array([1, 7, 30, 90])
    y_data = np.array([benchmark["d1"], benchmark["d7"], benchmark["d30"], benchmark["d90"]])
    
    try:
        popt, _ = curve_fit(retention_curve, x_data, y_data, p0=[0.5, -0.3], maxfev=5000)
        a, b = popt
    except:
        a, b = benchmark["d1"], -0.5  # ê¸°ë³¸ê°’
    
    curve = []
    for day in range(1, days + 1):
        ret = a * np.power(day, b)
        ret = max(min(ret, 1.0), 0.001)
        curve.append(ret)
    
    return curve

def calculate_blended_retention(
    internal_curve: List[float],
    benchmark_curve: List[float],
    weight_internal: float
) -> List[float]:
    """ë‚´ë¶€ í‘œë³¸ê³¼ ë²¤ì¹˜ë§ˆí¬ë¥¼ ë¸”ë Œë”©í•œ ë¦¬í…ì…˜ ì»¤ë¸Œ ìƒì„±"""
    weight_benchmark = 1 - weight_internal
    
    blended = []
    for i in range(len(internal_curve)):
        internal_val = internal_curve[i] if i < len(internal_curve) else internal_curve[-1]
        benchmark_val = benchmark_curve[i] if i < len(benchmark_curve) else benchmark_curve[-1]
        blended_val = (internal_val * weight_internal) + (benchmark_val * weight_benchmark)
        blended.append(max(min(blended_val, 1.0), 0.001))
    
    return blended

def calculate_blended_pr(
    internal_pr: List[float],
    benchmark_pr: float,
    weight_internal: float,
    days: int = 365
) -> List[float]:
    """PR ë¸”ë Œë”©"""
    weight_benchmark = 1 - weight_internal
    
    blended = []
    for i in range(days):
        internal_val = internal_pr[i] if i < len(internal_pr) else internal_pr[-1]
        blended_val = (internal_val * weight_internal) + (benchmark_pr * weight_benchmark)
        blended.append(max(min(blended_val, 1.0), 0.001))
    
    return blended

def calculate_blended_arppu(
    internal_arppu: List[float],
    benchmark_arppu: float,
    weight_internal: float,
    days: int = 365
) -> List[float]:
    """ARPPU ë¸”ë Œë”©"""
    weight_benchmark = 1 - weight_internal
    
    blended = []
    for i in range(days):
        internal_val = internal_arppu[i] if i < len(internal_arppu) else internal_arppu[-1]
        blended_val = (internal_val * weight_internal) + (benchmark_arppu * weight_benchmark)
        blended.append(max(blended_val, 1000))
    
    return blended

class AIInsightRequest(BaseModel):
    projection_summary: Dict[str, Any]
    analysis_type: str = "general"  # general, retention, nru, revenue, risk

# Retention Curve: a * (day)^b
def retention_curve(x, a, b):
    return a * np.power(x, b)

def fit_retention_curve(retention_data: List[float]):
    days = np.arange(1, len(retention_data) + 1)
    retention = np.array(retention_data)
    
    valid_mask = (retention > 0) & (retention <= 1)
    if np.sum(valid_mask) < 3:
        return None, None
    
    try:
        popt, _ = curve_fit(
            retention_curve, 
            days[valid_mask], 
            retention[valid_mask],
            p0=[retention_data[0], -0.5],
            bounds=([0, -2], [2, 0]),
            maxfev=5000
        )
        return popt[0], popt[1]
    except:
        return retention_data[0], -0.5

def calculate_retention_coefficients(selected_games: List[str], raw_data: dict):
    retention_games = raw_data['games']['retention']
    
    a_values = []
    b_values = []
    
    for game in selected_games:
        if game in retention_games:
            a, b = fit_retention_curve(retention_games[game])
            if a is not None:
                a_values.append(a)
                b_values.append(b)
    
    if not a_values:
        return 1.0, -0.5
    
    return np.mean(a_values), np.mean(b_values)

def generate_retention_curve(a: float, b: float, target_d1: float, days: int = 365):
    base_d1 = retention_curve(1, a, b)
    if base_d1 > 0:
        scale_factor = target_d1 / base_d1
    else:
        scale_factor = 1.0
    
    curve = []
    for day in range(1, days + 1):
        ret = retention_curve(day, a, b) * scale_factor
        curve.append(min(max(ret, 0.001), 1))
    
    return curve

def calculate_nru_pattern(selected_games: List[str], raw_data: dict):
    nru_games = raw_data['games']['nru']
    
    valid_games = [g for g in selected_games if g in nru_games]
    if not valid_games:
        return [0.98 ** i for i in range(365)]
    
    min_len = min(len(nru_games[g]) for g in valid_games)
    min_len = min(min_len, 365)
    
    daily_ratios = []
    for day in range(1, min_len):
        day_ratios = []
        for game in valid_games:
            data = nru_games[game]
            if day < len(data) and data[day-1] > 0:
                ratio = data[day] / data[day-1]
                if 0 < ratio < 2:
                    day_ratios.append(ratio)
        if day_ratios:
            daily_ratios.append(np.mean(day_ratios))
        else:
            daily_ratios.append(0.98)
    
    while len(daily_ratios) < 364:
        daily_ratios.append(daily_ratios[-1] if daily_ratios else 0.98)
    
    return daily_ratios

def generate_nru_series(total_nru: int, daily_ratios: List[float], days: int = 365, 
                         launch_period: int = 30, sustaining_ratio: float = 0.1):
    """
    NRU ì‹œë¦¬ì¦ˆ ìƒì„± - ëŸ°ì¹­ ë§ˆì¼€íŒ…ì€ D1~D30ì— ì§‘ì¤‘
    
    ğŸ”¥ V8.3 ìˆ˜ì •: Area Normalization ì ìš©
    - total_nru: ëŸ°ì¹­ ê¸°ê°„ ë™ì•ˆì˜ "ì´ ëª¨ê° ìˆ˜" (ì˜ˆì‚°/CPIë¡œ ê³„ì‚°ëœ ê°’)
    - ì´ ì´ëŸ‰ì„ 30ì¼ íŒ¨í„´ì˜ ë©´ì (Area)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ D1 ë†’ì´(Scale)ë¥¼ ì‚°ì¶œ
    - ê²°ê³¼: ì˜ˆì‚° ë²”ìœ„ ë‚´ì—ì„œ ìœ ì €ê°€ ë¶„ì‚° ìœ ì…ë¨
    
    Args:
        total_nru: ëŸ°ì¹­ ê¸°ê°„ ì´ ëª¨ê° ìˆ˜ (ğŸ”¥ ê¸°ì¡´ d1_nru â†’ total_nruë¡œ í•´ì„ ë³€ê²½)
        daily_ratios: ì¼ë³„ ê°ì†Œ ë¹„ìœ¨ (í˜„ì¬ ë¯¸ì‚¬ìš©, í™•ì¥ìš©)
        days: í”„ë¡œì ì…˜ ê¸°ê°„
        launch_period: ëŸ°ì¹­ ë§ˆì¼€íŒ… ì§‘ì¤‘ ê¸°ê°„ (ê¸°ë³¸ 30ì¼)
        sustaining_ratio: ëŸ°ì¹­ í›„ ìœ ì§€ NRU ë¹„ìœ¨ (ê¸°ë³¸ 10%)
    
    Returns:
        ì¼ë³„ NRU ë¦¬ìŠ¤íŠ¸
    """
    nru_series = []
    
    # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: Area Normalization
    # Step 1: ëŸ°ì¹­ ê¸°ê°„ NRU íŒ¨í„´ ìƒì„± (Power Law Decay: 1/t^0.8)
    nru_decay_pattern = []
    for t in range(1, launch_period + 1):
        decay_value = 1.0 / (t ** 0.8)  # D1=1.0, D2=0.57, D3=0.44, ...
        nru_decay_pattern.append(decay_value)
    
    # Step 2: íŒ¨í„´ì˜ ë©´ì (Area) ê³„ì‚° - ì´ëŸ‰ ë³´ì¡´ì˜ ë²•ì¹™!
    pattern_area = sum(nru_decay_pattern)
    
    # Step 3: D1 Scale Factor = ì´ ìœ ì € ìˆ˜ / íŒ¨í„´ ë©´ì 
    # ì´ë ‡ê²Œ í•˜ë©´ ëŸ°ì¹­ ê¸°ê°„ NRUì˜ í•© = total_nruê°€ ë¨
    d1_scale = total_nru / pattern_area if pattern_area > 0 else 0
    
    # Phase 1: ëŸ°ì¹­ ê¸°ê°„ (D1~D30) - ì •ê·œí™”ëœ íŒ¨í„´ ì ìš©
    for day in range(min(launch_period, days)):
        # ì •ê·œí™”ëœ NRU = Scale Ã— íŒ¨í„´ê°’
        daily_nru = int(d1_scale * nru_decay_pattern[day])
        nru_series.append(max(daily_nru, 10))  # ìµœì†Œê°’ 10ìœ¼ë¡œ ì„¤ì •
    
    # Phase 2: ëŸ°ì¹­ í›„ ìœ ì§€ ê¸°ê°„ (D31~D365)
    # D30ì˜ NRUë¥¼ ê¸°ì¤€ìœ¼ë¡œ sustaining_ratioë§Œí¼ ìœ ì§€
    d30_nru = nru_series[-1] if nru_series else 100
    sustaining_nru = int(d30_nru * sustaining_ratio * 10)  # D30ì˜ ~100% ìˆ˜ì¤€ì—ì„œ ì‹œì‘
    
    for day in range(launch_period, days):
        # ìœ ì§€ ê¸°ê°„ì—ë„ ì„œì„œíˆ ê°ì†Œ (ì›” 5% ê°ì†Œ)
        months_after_launch = (day - launch_period) / 30
        decay = np.exp(-0.05 * months_after_launch)
        daily_nru = int(sustaining_nru * decay)
        nru_series.append(max(daily_nru, 10))
    
    return nru_series[:days]

def calculate_pr_pattern(selected_games: List[str], raw_data: dict):
    pr_games = raw_data['games']['payment_rate']
    
    valid_games = [g for g in selected_games if g in pr_games]
    if not valid_games:
        return [0.02] * 365
    
    min_len = min(len(pr_games[g]) for g in valid_games)
    min_len = min(min_len, 365)
    
    pattern = []
    for day in range(min_len):
        day_values = [pr_games[g][day] for g in valid_games if day < len(pr_games[g]) and pr_games[g][day] > 0]
        avg_pr = np.mean(day_values) if day_values else 0.02
        pattern.append(max(avg_pr, 0.001))
    
    while len(pattern) < 365:
        pattern.append(pattern[-1] if pattern else 0.02)
    
    return pattern[:365]

def calculate_arppu_pattern(selected_games: List[str], raw_data: dict):
    arppu_games = raw_data['games']['arppu']
    
    valid_games = [g for g in selected_games if g in arppu_games]
    if not valid_games:
        return [50000] * 365
    
    min_len = min(len(arppu_games[g]) for g in valid_games)
    min_len = min(min_len, 365)
    
    pattern = []
    for day in range(min_len):
        day_values = [arppu_games[g][day] for g in valid_games if day < len(arppu_games[g]) and arppu_games[g][day] > 0]
        avg_arppu = np.mean(day_values) if day_values else 50000
        pattern.append(max(avg_arppu, 1000))
    
    while len(pattern) < 365:
        pattern.append(pattern[-1] if pattern else 50000)
    
    return pattern[:365]

def calculate_dau_matrix(nru_series: List[int], retention_curve: List[float], days: int = 365):
    daily_dau = []
    
    for active_day in range(days):
        total_dau = 0
        for cohort_day in range(active_day + 1):
            days_since_install = active_day - cohort_day
            if cohort_day < len(nru_series) and days_since_install < len(retention_curve):
                nru = nru_series[cohort_day]
                retention = retention_curve[days_since_install]
                total_dau += nru * retention
        daily_dau.append(int(total_dau))
    
    return daily_dau

def calculate_revenue(dau: List[float], pr: List[float], arppu: List[float]):
    """
    ì¼ë³„ ë§¤ì¶œ ê³„ì‚°
    
    Revenue = DAU Ã— PR Ã— (ARPPU / 30)
    
    ì£¼ì˜: ARPPUëŠ” 'ì›”ê°„' ê²°ì œìë‹¹ í‰ê·  ê²°ì œì•¡ì´ë¯€ë¡œ,
          ì¼ë³„ ê³„ì‚° ì‹œ 30ìœ¼ë¡œ ë‚˜ëˆ ì•¼ í•¨
    """
    revenue = []
    for i in range(len(dau)):
        pr_val = pr[i] if i < len(pr) else pr[-1]
        arppu_val = arppu[i] if i < len(arppu) else arppu[-1]
        
        # ARPPUë¥¼ ì¼ë³„ë¡œ í™˜ì‚° (ì›”ê°„ ARPPU / 30)
        daily_arppu = arppu_val / 30
        
        # ì¼ë³„ ë§¤ì¶œ = DAU Ã— PR Ã— ì¼ë³„ ARPPU
        daily_revenue = dau[i] * pr_val * daily_arppu
        revenue.append(daily_revenue)
    
    return revenue

# Claude AI Integration
async def get_claude_insight(prompt: str) -> str:
    """Call Claude API for AI insights"""
    if not CLAUDE_API_KEY:
        return "AI ì¸ì‚¬ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ CLAUDE_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                CLAUDE_API_URL,
                headers={
                    "x-api-key": CLAUDE_API_KEY,
                    "anthropic-version": "2023-06-01",
                    "content-type": "application/json"
                },
                json={
                    "model": "claude-sonnet-4-20250514",
                    "max_tokens": 1024,
                    "messages": [
                        {"role": "user", "content": prompt}
                    ]
                },
                timeout=60.0
            )
            
            if response.status_code == 200:
                data = response.json()
                return data["content"][0]["text"]
            else:
                error_detail = response.json().get("error", {}).get("message", response.status_code)
                return f"AI ë¶„ì„ ì˜¤ë¥˜: {error_detail}"
    except Exception as e:
        return f"AI ì—°ê²° ì‹¤íŒ¨: {str(e)}"

def create_insight_prompt(summary: Dict[str, Any], analysis_type: str) -> str:
    """Create prompt for Claude based on analysis type with Multi-Persona approach"""
    
    # V7 ì„¤ì • ì •ë³´ ì¶”ì¶œ
    v7_settings = summary.get('v7_settings', {})
    blending = summary.get('blending', {})
    
    base_context = f"""ë‹¹ì‹ ì€ ê²Œì„ KPI í”„ë¡œì ì…˜ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” 4ëª…ì˜ ì „ë¬¸ê°€ íŒ¨ë„ì…ë‹ˆë‹¤.

[V7 ì „ë¬¸ê°€ íŒ¨ë„ êµ¬ì„±]
1. ê¸€ë¡œë²Œ ë§ˆì¼€íŒ… ë° UA ì „ë¬¸ê°€: CPI ì ì •ì„±, ëª¨ê° íš¨ìœ¨, UA ì „ëµ, CAC/LTV ë¶„ì„
2. ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸: ì§€í‘œ ê±´ì „ì„±, ë¦¬í…ì…˜ íŒ¨í„´, í†µê³„ì  ì‹ ë¢°ë„, ì˜ˆì¸¡ ì •í™•ë„
3. ê¸€ë¡œë²Œ í¼ë¸”ë¦¬ì‹± ì „ë¬¸ê°€: BM êµ¬ì¡°, ì‹œì¥ ê²½ìŸë ¥, ì¥ë¥´ íŠ¹ì„±, ê¸€ë¡œë²Œ íŠ¸ë Œë“œ
4. ì¬ë¬´ ì„¤ê³„ ê±´ì „ì„± ì „ë¬¸ê°€: BEP, ROAS, íˆ¬ì íšŒìˆ˜, í˜„ê¸ˆíë¦„, ë¦¬ìŠ¤í¬

[í”„ë¡œì ì…˜ ê²°ê³¼ ìš”ì•½]
í”„ë¡œì ì…˜ ê¸°ê°„: {summary.get('projection_days', 365)}ì¼
ëŸ°ì¹­ì¼: {summary.get('launch_date', 'N/A')}

Best ì‹œë‚˜ë¦¬ì˜¤:
- ì´ Gross Revenue: {summary.get('best', {}).get('gross_revenue', 0):,.0f}ì›
- ì´ NRU: {summary.get('best', {}).get('total_nru', 0):,}ëª…
- Peak DAU: {summary.get('best', {}).get('peak_dau', 0):,}ëª…
- í‰ê·  DAU: {summary.get('best', {}).get('average_dau', 0):,}ëª…

Normal ì‹œë‚˜ë¦¬ì˜¤:
- ì´ Gross Revenue: {summary.get('normal', {}).get('gross_revenue', 0):,.0f}ì›
- ì´ NRU: {summary.get('normal', {}).get('total_nru', 0):,}ëª…
- Peak DAU: {summary.get('normal', {}).get('peak_dau', 0):,}ëª…
- í‰ê·  DAU: {summary.get('normal', {}).get('average_dau', 0):,}ëª…

Worst ì‹œë‚˜ë¦¬ì˜¤:
- ì´ Gross Revenue: {summary.get('worst', {}).get('gross_revenue', 0):,.0f}ì›
- ì´ NRU: {summary.get('worst', {}).get('total_nru', 0):,}ëª…
- Peak DAU: {summary.get('worst', {}).get('peak_dau', 0):,}ëª…
- í‰ê·  DAU: {summary.get('worst', {}).get('average_dau', 0):,}ëª…

[V7 ì‚°ìˆ  ê·¼ê±° - ì´ ê²°ê³¼ê°€ ì–´ë–»ê²Œ ë„ì¶œë˜ì—ˆëŠ”ì§€]
- ë¸”ë Œë”© ë¹„ìœ¨: ë‚´ë¶€ í‘œë³¸ {blending.get('weight_internal', 0.7)*100:.0f}% + ë²¤ì¹˜ë§ˆí¬ {blending.get('weight_benchmark', 0.3)*100:.0f}%
- Time-Decay: {blending.get('time_decay', True)} (D1:ë‚´ë¶€90% â†’ D365:ë²¤ì¹˜ë§ˆí¬90%)
- í’ˆì§ˆ ë“±ê¸‰: {v7_settings.get('quality_score', 'B')}ê¸‰ (ìŠ¹ìˆ˜ Ã—{v7_settings.get('quality_multiplier', 1.0)})
- BM íƒ€ì…: {v7_settings.get('bm_type', 'Midcore')}
- ì ìš© ì§€ì—­: {', '.join(v7_settings.get('regions', ['global']))}
- ê³„ì ˆì„± ì ìš©: {v7_settings.get('seasonality_applied', True)}
- ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€: {blending.get('genre', 'N/A')} / {', '.join(blending.get('platforms', ['PC']))}

[ì¤‘ìš” ì§€ì‹œì‚¬í•­]
- ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•(###, **, -, * ë“±)ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”
- ë²ˆí˜¸ëŠ” 1. 2. 3. í˜•ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”
- ê°•ì¡°ëŠ” ë”°ì˜´í‘œë‚˜ ê´„í˜¸ë¡œ í‘œí˜„í•˜ì„¸ìš”
- ê²½ì˜ì§„ ë³´ê³ ìš©ìœ¼ë¡œ ì „ë¬¸ì ì´ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
"""
    
    type_prompts = {
        "executive_report": f"""
[ë¶„ì„ ìš”ì²­: ê²½ì˜ì§„ìš© ì¢…í•© ë³´ê³ ì„œ]
4ëª…ì˜ ì „ë¬¸ê°€ê°€ ê°ìì˜ ê´€ì ì—ì„œ ë¶„ì„í•˜ê³ , ìµœì¢… ê²½ì˜ ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ ì¢…í•© ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:

[1. Executive Summary - í•µì‹¬ ìš”ì•½]
Normal ì‹œë‚˜ë¦¬ì˜¤ ê¸°ì¤€ 1ë…„ ì˜ˆìƒ ë§¤ì¶œê³¼ í•µì‹¬ ì§€í‘œë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½

[2. ì‚°ìˆ  ê·¼ê±° ë° ê°€ì •]
- ì´ í”„ë¡œì ì…˜ì´ ì–´ë–¤ ê°€ì •ê³¼ ë¡œì§ìœ¼ë¡œ ë„ì¶œë˜ì—ˆëŠ”ì§€ ì„¤ëª…
- ë¸”ë Œë”© ë¹„ìœ¨, í’ˆì§ˆ ë“±ê¸‰, BM íƒ€ì…ì´ ê²°ê³¼ì— ë¯¸ì¹œ ì˜í–¥

[3. ì „ë¬¸ê°€ íŒ¨ë„ ë¶„ì„]
(1) UA ì „ë¬¸ê°€: CPI {summary.get('cpi', 'N/A')}ì› ê¸°ì¤€ ëª¨ê° íš¨ìœ¨ í‰ê°€, NRU ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥ì„±
(2) ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸: ë¦¬í…ì…˜ ì»¤ë¸Œ ê±´ì „ì„±, Best-Worst í¸ì°¨ {((summary.get('best', {}).get('gross_revenue', 1) / max(summary.get('worst', {}).get('gross_revenue', 1), 1) - 1) * 100):.0f}% ì ì •ì„±
(3) í¼ë¸”ë¦¬ì‹± ì „ë¬¸ê°€: {blending.get('genre', 'N/A')} ì¥ë¥´ ì‹œì¥ ëŒ€ë¹„ ê²½ìŸë ¥, BM êµ¬ì¡° ì í•©ì„±
(4) ì¬ë¬´ ì „ë¬¸ê°€: ROAS, ì†ìµë¶„ê¸°ì  ë„ë‹¬ ì˜ˆìƒ, íˆ¬ì ë¦¬ìŠ¤í¬

[4. Go/No-Go ê¶Œê³ ]
- ìµœì¢… ê¶Œê³ : (Go / Conditional Go / No-Go ì¤‘ í•˜ë‚˜)
- ê¶Œê³  ì´ìœ : í•œ ë¬¸ì¥
- í•µì‹¬ ë¦¬ìŠ¤í¬ 3ê°€ì§€
- ê¶Œì¥ ì•¡ì…˜ 3ê°€ì§€

ì´ 800ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
        "general": """
[ë¶„ì„ ìš”ì²­: ì¢…í•© ë¶„ì„]
4ëª…ì˜ ì „ë¬¸ê°€ê°€ ê°ìì˜ ê´€ì ì—ì„œ í•µì‹¬ ì˜ê²¬ì„ ì œì‹œí•˜ê³ , ìµœì¢… ì¢…í•© ê²°ë¡ ì„ ë„ì¶œí•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
1. UA ì „ë¬¸ê°€ ì˜ê²¬: (ëª¨ê° íš¨ìœ¨ ê´€ì ì—ì„œ í•œ ë¬¸ì¥)
2. ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ì˜ê²¬: (ì§€í‘œ ê±´ì „ì„± ê´€ì ì—ì„œ í•œ ë¬¸ì¥)  
3. í¼ë¸”ë¦¬ì‹± ì „ë¬¸ê°€ ì˜ê²¬: (ì‹œì¥ ê²½ìŸë ¥ ê´€ì ì—ì„œ í•œ ë¬¸ì¥)
4. ì¬ë¬´ ì „ë¬¸ê°€ ì˜ê²¬: (íˆ¬ì íšŒìˆ˜ ê´€ì ì—ì„œ í•œ ë¬¸ì¥)
5. ì¢…í•© ê²°ë¡ : 4ëª…ì˜ ì˜ê²¬ì„ ì¢…í•©í•œ ìµœì¢… í‰ê°€ì™€ ê¶Œì¥ ì•¡ì…˜ 3ê°€ì§€

ì´ 400ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
        "reliability": """
[ë¶„ì„ ìš”ì²­: ì‹ ë¢°ë„ í‰ê°€]
4ëª…ì˜ ì „ë¬¸ê°€ê°€ ì´ í”„ë¡œì ì…˜ì˜ ì‹ ë¢°ë„ë¥¼ ê°ìì˜ ê´€ì ì—ì„œ í‰ê°€í•˜ê³  ì¢…í•© ì ìˆ˜ë¥¼ ì‚°ì •í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
1. ì‹ ë¢°ë„ ì ìˆ˜: (100ì  ë§Œì , ìˆ«ìë§Œ)
2. ì‹ ë¢°ë„ ë“±ê¸‰: (A/B/C/D/F ì¤‘ í•˜ë‚˜)
3. ì „ë¬¸ê°€ë³„ í‰ê°€
   - UA ì „ë¬¸ê°€: (NRU/CPI ëª©í‘œ í˜„ì‹¤ì„± í‰ê°€)
   - ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸: (í‘œë³¸ ë°ì´í„° í’ˆì§ˆ, ì‹œë‚˜ë¦¬ì˜¤ í¸ì°¨ ì ì •ì„± í‰ê°€)
   - í¼ë¸”ë¦¬ì‹± ì „ë¬¸ê°€: (ì‹œì¥ ëŒ€ë¹„ ë²¤ì¹˜ë§ˆí¬ ì ì •ì„± í‰ê°€)
   - ì¬ë¬´ ì „ë¬¸ê°€: (ìˆ˜ìµ ì˜ˆì¸¡ í˜„ì‹¤ì„± í‰ê°€)
4. ì‹ ë¢°ë„ í–¥ìƒ ì œì•ˆ: êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆ 3ê°€ì§€

ì´ 500ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
        "retention": """
[ë¶„ì„ ìš”ì²­: ë¦¬í…ì…˜ ë¶„ì„]
4ëª…ì˜ ì „ë¬¸ê°€ê°€ ë¦¬í…ì…˜ ë° DAU íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
1. DAU íŒ¨í„´ ê±´ê°•ë„: (ì¢‹ìŒ/ë³´í†µ/ìš°ë ¤ ì¤‘ í•˜ë‚˜ì™€ ì´ìœ )
2. ì „ë¬¸ê°€ë³„ ë¦¬í…ì…˜ ì¸ì‚¬ì´íŠ¸
   - ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸: (ë¦¬í…ì…˜ ì»¤ë¸Œ ë¶„ì„, Power Law ì í•©ë„)
   - í¼ë¸”ë¦¬ì‹± ì „ë¬¸ê°€: (ì¥ë¥´ ëŒ€ë¹„ ë¦¬í…ì…˜ ìˆ˜ì¤€ í‰ê°€)
   - UA ì „ë¬¸ê°€: (ë¦¬í…ì…˜ ê°œì„ ì´ UA íš¨ìœ¨ì— ë¯¸ì¹˜ëŠ” ì˜í–¥)
3. ë¦¬í…ì…˜ ê°œì„  ì•¡ì…˜ í”Œëœ: ìš°ì„ ìˆœìœ„ë³„ 3ê°€ì§€

ì´ 400ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
        "revenue": """
[ë¶„ì„ ìš”ì²­: ë§¤ì¶œ ë¶„ì„]
4ëª…ì˜ ì „ë¬¸ê°€ê°€ ë§¤ì¶œ ì˜ˆì¸¡ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
1. ë§¤ì¶œ ì˜ˆì¸¡ í˜„ì‹¤ì„±: (ë‚™ê´€ì /ì ì •/ë³´ìˆ˜ì  ì¤‘ í•˜ë‚˜ì™€ ì´ìœ )
2. ì „ë¬¸ê°€ë³„ ë§¤ì¶œ ì¸ì‚¬ì´íŠ¸
   - ì¬ë¬´ ì „ë¬¸ê°€: (ì†ìµë¶„ê¸°ì  ë° íˆ¬ìíšŒìˆ˜ ê´€ì )
   - UA ì „ë¬¸ê°€: (ARPU ë° ê³¼ê¸ˆ ì „í™˜ìœ¨ ê´€ì )
   - í¼ë¸”ë¦¬ì‹± ì „ë¬¸ê°€: (ì‹œì¥ ê·œëª¨ ëŒ€ë¹„ ì ìœ ìœ¨ ê´€ì )
3. ë§¤ì¶œ ê·¹ëŒ€í™” ì „ëµ: ìš°ì„ ìˆœìœ„ë³„ 3ê°€ì§€

ì´ 400ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
        "risk": """
[ë¶„ì„ ìš”ì²­: ë¦¬ìŠ¤í¬ ë¶„ì„]
4ëª…ì˜ ì „ë¬¸ê°€ê°€ ë¦¬ìŠ¤í¬ ìš”ì¸ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
1. ì „ì²´ ë¦¬ìŠ¤í¬ ìˆ˜ì¤€: (ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ ì¤‘ í•˜ë‚˜)
2. Best-Worst í¸ì°¨ ë¶„ì„: (í¸ì°¨ ë¹„ìœ¨ê³¼ ì˜ë¯¸)
3. ì „ë¬¸ê°€ë³„ ë¦¬ìŠ¤í¬ ì‹ë³„
   - ì¬ë¬´ ì „ë¬¸ê°€: (ì¬ë¬´ ë¦¬ìŠ¤í¬)
   - UA ì „ë¬¸ê°€: (UA ë¦¬ìŠ¤í¬)
   - ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸: (ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± ë¦¬ìŠ¤í¬)
   - í¼ë¸”ë¦¬ì‹± ì „ë¬¸ê°€: (ì‹œì¥/ê²½ìŸ ë¦¬ìŠ¤í¬)
4. ë¦¬ìŠ¤í¬ ì™„í™” ì „ëµ: ìš°ì„ ìˆœìœ„ë³„ 3ê°€ì§€

ì´ 450ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
""",
        "competitive": """
[ë¶„ì„ ìš”ì²­: ê²½ìŸë ¥ ë¶„ì„]
4ëª…ì˜ ì „ë¬¸ê°€ê°€ ì‹œì¥ ê²½ìŸë ¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
1. ì‹œì¥ ê²½ìŸë ¥ ë“±ê¸‰: (ìƒ/ì¤‘/í•˜ ì¤‘ í•˜ë‚˜ì™€ ì´ìœ )
2. ì „ë¬¸ê°€ë³„ ê²½ìŸë ¥ í‰ê°€
   - í¼ë¸”ë¦¬ì‹± ì „ë¬¸ê°€: (ì¥ë¥´ ë‚´ í¬ì§€ì…”ë‹)
   - UA ì „ë¬¸ê°€: (ì°¨ë³„í™” í¬ì¸íŠ¸)
   - ì¬ë¬´ ì „ë¬¸ê°€: (ìˆ˜ìµ ëª¨ë¸ ê²½ìŸë ¥)
3. ê²½ìŸë ¥ ê°•í™” ì „ëµ: ìš°ì„ ìˆœìœ„ë³„ 3ê°€ì§€

ì´ 400ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
    }
    
    return base_context + type_prompts.get(analysis_type, type_prompts["general"])

# API Endpoints
@app.get("/")
async def root():
    return {"message": "Game KPI Projection API", "version": "2.0.0", "ai_enabled": bool(CLAUDE_API_KEY)}

@app.get("/api/games")
async def get_available_games():
    raw_data = load_raw_data()
    return {
        "retention": list(raw_data['games']['retention'].keys()),
        "nru": list(raw_data['games']['nru'].keys()),
        "payment_rate": list(raw_data['games']['payment_rate'].keys()),
        "arppu": list(raw_data['games']['arppu'].keys())
    }

@app.get("/api/games/metadata")
async def get_games_metadata():
    """Get metadata for all games (release date, genre, platform, etc.)"""
    raw_data = load_raw_data()
    return raw_data.get('game_metadata', {})

@app.get("/api/games/{metric}/{game_name}")
async def get_game_data(metric: str, game_name: str):
    raw_data = load_raw_data()
    
    if metric not in raw_data['games']:
        raise HTTPException(status_code=404, detail=f"Metric '{metric}' not found")
    
    if game_name not in raw_data['games'][metric]:
        raise HTTPException(status_code=404, detail=f"Game '{game_name}' not found in {metric}")
    
    return {
        "game": game_name,
        "metric": metric,
        "data": raw_data['games'][metric][game_name]
    }

@app.get("/api/config")
async def get_default_config():
    return load_config()

@app.post("/api/projection")
async def calculate_projection(input_data: ProjectionInput):
    raw_data = load_raw_data()
    
    days = input_data.projection_days
    results = {"best": {}, "normal": {}, "worst": {}}
    
    # ============================================
    # V7: ë¸”ë Œë”© ì„¤ì • ì¶”ì¶œ
    # ============================================
    blending = input_data.blending or {}
    base_weight = blending.get("weight", 0.7)  # ê¸°ë³¸ê°’: ë‚´ë¶€ 70%
    genre = blending.get("genre", "MMORPG")
    platforms = blending.get("platforms", ["PC"])
    use_benchmark_only = blending.get("benchmark_only", False)
    use_time_decay = blending.get("time_decay", True)  # V7: Time-Decay ê¸°ë³¸ í™œì„±í™”
    
    # V7: Quality Score & BM Type
    quality_grade = input_data.quality_score or "B"
    quality_multiplier = QUALITY_SCORES.get(quality_grade, 1.0)
    bm_type = input_data.bm_type or "Midcore"
    bm_modifier = BM_TYPE_MODIFIERS.get(bm_type, {"pr_mod": 1.0, "arppu_mod": 1.0})
    
    # V7: ê³„ì ˆì„± íŒ©í„°
    regions = input_data.regions or ["global"]
    seasonality_factors = calculate_seasonality(regions, input_data.launch_date, days)
    
    # í‘œë³¸ ê²Œì„ì´ ì—†ìœ¼ë©´ ë²¤ì¹˜ë§ˆí¬ 100% ì‚¬ìš©
    has_sample_games = len(input_data.retention.selected_games) > 0
    if not has_sample_games:
        base_weight = 0.0
        use_benchmark_only = True
    
    # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (BM Type ì ìš©)
    benchmark = get_benchmark_data(genre, platforms)
    benchmark["pr"] = benchmark["pr"] * bm_modifier["pr_mod"]
    benchmark["arppu"] = benchmark["arppu"] * bm_modifier["arppu_mod"]
    benchmark_ret_curve = generate_benchmark_retention_curve(benchmark, days)
    
    # ë‚´ë¶€ í‘œë³¸ ê¸°ë°˜ ê³„ìˆ˜ ê³„ì‚°
    a, b = calculate_retention_coefficients(input_data.retention.selected_games, raw_data)
    pr_pattern = calculate_pr_pattern(input_data.revenue.selected_games_pr, raw_data)
    arppu_pattern = calculate_arppu_pattern(input_data.revenue.selected_games_arppu, raw_data)
    
    for scenario in ["best", "normal", "worst"]:
        target_d1 = input_data.retention.target_d1_retention[scenario]
        
        # ë‚´ë¶€ í‘œë³¸ ê¸°ë°˜ ë¦¬í…ì…˜ ì»¤ë¸Œ
        internal_ret_curve = generate_retention_curve(a, b, target_d1, days)
        
        # V7: Time-Decay ë¸”ë Œë”© ì ìš©
        if use_time_decay and not use_benchmark_only:
            # ë²¤ì¹˜ë§ˆí¬ ì»¤ë¸Œë¥¼ target_d1ì— ë§ê²Œ ìŠ¤ì¼€ì¼ë§
            benchmark_scale = target_d1 / benchmark["d1"] if benchmark["d1"] > 0 else 1.0
            scaled_benchmark_curve = [min(r * benchmark_scale, 1.0) for r in benchmark_ret_curve]
            ret_curve = calculate_time_decay_blended_retention(
                internal_ret_curve, scaled_benchmark_curve, days, quality_multiplier
            )
        elif not use_benchmark_only:
            # ê¸°ì¡´ ê³ ì • ë¸”ë Œë”©
            benchmark_scale = target_d1 / benchmark["d1"] if benchmark["d1"] > 0 else 1.0
            scaled_benchmark_curve = [min(r * benchmark_scale, 1.0) for r in benchmark_ret_curve]
            ret_curve = calculate_blended_retention(internal_ret_curve, scaled_benchmark_curve, base_weight)
        else:
            # ë²¤ì¹˜ë§ˆí¬ë§Œ ì‚¬ìš©
            benchmark_scale = target_d1 / benchmark["d1"] if benchmark["d1"] > 0 else 1.0
            ret_curve = [min(r * benchmark_scale * quality_multiplier, 1.0) for r in benchmark_ret_curve]
        
        # V7: NRU ì‹œë¦¬ì¦ˆ ìƒì„± (ëŸ°ì¹­ ë§ˆì¼€íŒ… D1~D30 ì§‘ì¤‘)
        d1_nru = input_data.nru.d1_nru[scenario]
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ NRU ë³´ì •
        nru_adj = input_data.nru.adjustment.get("best_vs_normal", 0) if scenario == "best" else \
                  input_data.nru.adjustment.get("worst_vs_normal", 0) if scenario == "worst" else 0
        adjusted_d1_nru = int(d1_nru * (1 + nru_adj))
        
        nru_series = generate_nru_series(adjusted_d1_nru, [], days)
        
        # V7: ê³„ì ˆì„± ì ìš© (NRUì— ë°˜ì˜)
        nru_series = [int(nru * sf) for nru, sf in zip(nru_series, seasonality_factors)]
        
        # DAU ê³„ì‚°
        dau_series = calculate_dau_matrix(nru_series, ret_curve, days)
        
        # PR ë³´ì •
        pr_adj = input_data.revenue.pr_adjustment.get("best_vs_normal", 0) if scenario == "best" else \
                 input_data.revenue.pr_adjustment.get("worst_vs_normal", 0) if scenario == "worst" else 0
        
        # PR ë¸”ë Œë”© (BM Type ì ìš©ë¨) + V7: Quality Scoreë„ ì ìš©
        if not use_benchmark_only:
            weight_internal = base_weight
            # ë²¤ì¹˜ë§ˆí¬ PRì— Quality Score ì ìš©
            adjusted_benchmark_pr = benchmark["pr"] * quality_multiplier
            pr_series = calculate_blended_pr(pr_pattern, adjusted_benchmark_pr, weight_internal, days)
        else:
            # ë²¤ì¹˜ë§ˆí¬ë§Œ ì‚¬ìš© ì‹œì—ë„ Quality Score ì ìš©
            pr_series = [benchmark["pr"] * quality_multiplier] * days
        pr_series = [p * (1 + pr_adj) for p in pr_series]
        
        # ARPPU ë³´ì •
        arppu_adj = input_data.revenue.arppu_adjustment.get("best_vs_normal", 0) if scenario == "best" else \
                    input_data.revenue.arppu_adjustment.get("worst_vs_normal", 0) if scenario == "worst" else 0
        
        # ARPPU ë¸”ë Œë”© (BM Type ì ìš©ë¨) + V7: Quality Scoreë„ ì ìš©
        if not use_benchmark_only:
            # ë²¤ì¹˜ë§ˆí¬ ARPPUì— Quality Score ì ìš©
            adjusted_benchmark_arppu = benchmark["arppu"] * quality_multiplier
            arppu_series = calculate_blended_arppu(arppu_pattern, adjusted_benchmark_arppu, base_weight, days)
        else:
            # ë²¤ì¹˜ë§ˆí¬ë§Œ ì‚¬ìš© ì‹œì—ë„ Quality Score ì ìš©
            arppu_series = [benchmark["arppu"] * quality_multiplier] * days
        arppu_series = [a * (1 + arppu_adj) for a in arppu_series]
        
        # V7: ê³„ì ˆì„±ì„ ARPPUì—ë„ ë°˜ì˜
        arppu_series = [arppu * sf for arppu, sf in zip(arppu_series, seasonality_factors)]
        
        # Revenue ê³„ì‚° (ì¼ë³„ ARPPU í™˜ì‚° ì ìš©ë¨)
        revenue_series = calculate_revenue(dau_series, pr_series, arppu_series)
        
        results[scenario] = {
            "retention": {
                "coefficients": {"a": float(a), "b": float(b)},
                "target_d1": target_d1,
                "curve": ret_curve[:90]
            },
            "nru": {
                "d1_nru": d1_nru,
                "series": nru_series[:90],
                "total": sum(nru_series)
            },
            "dau": {
                "series": dau_series[:90],
                "peak": int(max(dau_series)),
                "average": int(np.mean(dau_series))
            },
            "revenue": {
                "pr_series": pr_series[:90],
                "arppu_series": arppu_series[:90],
                "daily_revenue": revenue_series[:90],
                "total_gross": sum(revenue_series),
                "average_daily": float(np.mean(revenue_series))
            },
            "full_data": {
                "nru": nru_series,
                "dau": dau_series,
                "revenue": revenue_series,
                "retention": ret_curve,
                "pr": pr_series,
                "arppu": arppu_series
            }
        }
    
    # Calculate summary
    summary = {}
    for scenario in ["best", "normal", "worst"]:
        basic = input_data.basic_settings or load_config()["basic_settings"]
        gross = results[scenario]["revenue"]["total_gross"]
        
        market_fee = basic.get("market_fee_ratio", 0.3)
        vat = basic.get("vat_ratio", 0.1)
        infra = basic.get("infrastructure_cost_ratio", 0.03)
        
        net = gross * (1 - market_fee - vat - infra)
        
        summary[scenario] = {
            "gross_revenue": gross,
            "net_revenue": net,
            "total_nru": results[scenario]["nru"]["total"],
            "peak_dau": results[scenario]["dau"]["peak"],
            "average_dau": results[scenario]["dau"]["average"],
            "average_daily_revenue": results[scenario]["revenue"]["average_daily"]
        }
    
    return {
        "status": "success",
        "input": {
            "launch_date": input_data.launch_date,
            "projection_days": days,
            "retention_games": input_data.retention.selected_games,
            "nru_games": input_data.nru.selected_games,
            "pr_games": input_data.revenue.selected_games_pr,
            "arppu_games": input_data.revenue.selected_games_arppu
        },
        "blending": {
            "weight_internal": base_weight,
            "weight_benchmark": 1 - base_weight,
            "time_decay": use_time_decay,
            "genre": genre,
            "platforms": platforms,
            "benchmark_only": use_benchmark_only,
            "benchmark_data": benchmark
        },
        "v7_settings": {
            "quality_score": quality_grade,
            "quality_multiplier": quality_multiplier,
            "bm_type": bm_type,
            "bm_modifier": bm_modifier,
            "regions": regions,
            "seasonality_applied": True
        },
        "summary": summary,
        "results": results
    }

# AI Insight Endpoint
@app.post("/api/ai/insight")
async def get_ai_insight(request: AIInsightRequest):
    """Get AI-powered insights for projection results"""
    prompt = create_insight_prompt(request.projection_summary, request.analysis_type)
    insight = await get_claude_insight(prompt)
    
    return {
        "status": "success",
        "analysis_type": request.analysis_type,
        "insight": insight,
        "ai_model": "gemini-1.5-flash"
    }

@app.get("/api/ai/status")
async def get_ai_status():
    """Check AI integration status"""
    return {
        "enabled": bool(CLAUDE_API_KEY),
        "model": "claude-sonnet-4",
        "available_types": ["general", "reliability", "retention", "revenue", "risk", "competitive"]
    }

@app.get("/api/raw-data")
async def get_raw_data():
    return load_raw_data()

@app.get("/api/raw-data/download")
async def download_raw_data_excel():
    """Download raw game data as Excel file (same format as original)"""
    import io
    from openpyxl import Workbook
    from openpyxl.styles import Font, PatternFill, Border, Side, Alignment
    from fastapi.responses import StreamingResponse
    
    raw_data = load_raw_data()
    wb = Workbook()
    
    # ìŠ¤íƒ€ì¼ ì •ì˜
    header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
    header_font = Font(color="FFFFFF", bold=True)
    thin_border = Border(
        left=Side(style='thin'),
        right=Side(style='thin'),
        top=Side(style='thin'),
        bottom=Side(style='thin')
    )
    
    def create_raw_sheet(ws, sheet_title, metric_name, description, data_dict):
        """Raw ë°ì´í„° ì‹œíŠ¸ ìƒì„± (ì›ë³¸ ì—‘ì…€ í˜•ì‹)"""
        # Row 1: ì•ˆë‚´ ë¬¸êµ¬
        ws['B1'] = f'- ì•„ë˜ ê²Œì„ ì¶”ê°€ ì‹œ {sheet_title} ê²Œì„ ë¦¬ìŠ¤íŠ¸ì— ìë™ìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤.'
        ws['B1'].font = Font(color="FF0000")
        
        # Row 2: ë©”íŠ¸ë¦­ëª… ë° ì„¤ëª…
        ws['B2'] = metric_name
        ws['B2'].font = Font(bold=True)
        ws['C2'] = description
        
        # Row 3: í—¤ë” (ê²Œì„ëª…, 1, 2, 3, ... 365)
        ws['B3'] = 'ê²Œì„ëª…'
        ws['B3'].fill = header_fill
        ws['B3'].font = header_font
        ws['B3'].border = thin_border
        
        max_days = 90 if metric_name == 'ë¦¬í…ì…˜' else 365
        for day in range(1, max_days + 1):
            col = day + 2  # Cë¶€í„° ì‹œì‘
            cell = ws.cell(row=3, column=col, value=day)
            cell.fill = header_fill
            cell.font = header_font
            cell.border = thin_border
        
        # Row 4+: ê²Œì„ ë°ì´í„°
        row_idx = 4
        for game_name, values in data_dict.items():
            ws.cell(row=row_idx, column=2, value=game_name).border = thin_border
            for i, val in enumerate(values[:max_days]):
                cell = ws.cell(row=row_idx, column=i + 3, value=val)
                cell.border = thin_border
                if metric_name in ['ë¦¬í…ì…˜', 'PR']:
                    cell.number_format = '0.00%'
            row_idx += 1
        
        # ì—´ ë„ˆë¹„ ì¡°ì •
        ws.column_dimensions['B'].width = 20
        for col in range(3, max_days + 3):
            ws.column_dimensions[ws.cell(row=3, column=col).column_letter].width = 8
    
    # Raw_Retention ì‹œíŠ¸
    ws_retention = wb.active
    ws_retention.title = "Raw_Retention"
    create_raw_sheet(ws_retention, "1. Retention", "ë¦¬í…ì…˜", "ë¡ ì¹­ ~ 90ì¼ê¹Œì§€ì˜ ë¦¬í…ì…˜ ì •ë³´ ì…ë ¥", raw_data['games'].get('retention', {}))
    
    # Raw_NRU ì‹œíŠ¸
    ws_nru = wb.create_sheet("Raw_NRU")
    create_raw_sheet(ws_nru, "2. NRU", "NRU", "ë¡ ì¹­ ~ 365ì¼ê¹Œì§€ì˜ ë°ì´í„° ì…ë ¥", raw_data['games'].get('nru', {}))
    
    # Raw_PR ì‹œíŠ¸
    ws_pr = wb.create_sheet("Raw_PR")
    create_raw_sheet(ws_pr, "3. Revenue", "PR", "ë¡ ì¹­ ~ 365ì¼ê¹Œì§€ì˜ ë°ì´í„° ì…ë ¥", raw_data['games'].get('payment_rate', {}))
    
    # Raw_ARPPU ì‹œíŠ¸
    ws_arppu = wb.create_sheet("Raw_ARPPU")
    create_raw_sheet(ws_arppu, "3. Revenue", "ARPPU", "ë¡ ì¹­ ~ 365ì¼ê¹Œì§€ì˜ ë°ì´í„° ì…ë ¥", raw_data['games'].get('arppu', {}))
    
    # ë©”ëª¨ë¦¬ì— ì €ì¥
    output = io.BytesIO()
    wb.save(output)
    output.seek(0)
    
    return StreamingResponse(
        output,
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        headers={"Content-Disposition": "attachment; filename=raw_game_data.xlsx"}
    )

@app.post("/api/raw-data/upload")
async def upload_game_data(file: UploadFile = File(...), metric: str = "retention"):
    if not file.filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="Only CSV files are supported")
    
    import pandas as pd
    from io import StringIO
    
    content = await file.read()
    df = pd.read_csv(StringIO(content.decode('utf-8')))
    
    raw_data = load_raw_data()
    
    for _, row in df.iterrows():
        game_name = row.iloc[0]
        values = row.iloc[1:].tolist()
        values = [float(v) for v in values if pd.notna(v)]
        
        if metric in raw_data['games']:
            raw_data['games'][metric][game_name] = values
    
    raw_data['metadata'][f'{metric}_games'] = list(raw_data['games'][metric].keys())
    
    with open(RAW_DATA_PATH, 'w', encoding='utf-8') as f:
        json.dump(raw_data, f, ensure_ascii=False, indent=2)
    
    return {"status": "success", "message": f"Added/updated games in {metric}"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
